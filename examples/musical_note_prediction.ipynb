{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of musical notes\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook adapts one reference experiment for note prediction using ESNs from ([https://arxiv.org/abs/1812.11527](https://arxiv.org/abs/1812.11527)) to PyRCN and shows that introducing bidirectional ESNs significantly improves the results in terms of Accuracy, already for rather small networks.\n",
    "\n",
    "The tutorial is based on numpy, scikit-learn, joblib and PyRCN. We are using the ESNRegressor, because we further process the outputs of the ESN. Note that the same can also be done using the ESNClassifier. Then, during prediction, we simply call \"predict_proba\".\n",
    "\n",
    "This tutorial requires the Python modules numpy, scikit-learn, matplotlib and pyrcn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from joblib import load\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['image.cmap'] = 'jet'\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "%matplotlib inline\n",
    "\n",
    "from pyrcn.echo_state_network import ESNRegressor\n",
    "from pyrcn.linear_model import IncrementalRegression\n",
    "from pyrcn.base import InputToNode, NodeToNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "The datasets are online available at ([http://www-etud.iro.umontreal.ca/~boulanni/icml2012](http://www-etud.iro.umontreal.ca/~boulanni/icml2012)). In this notebook, we use the pre-processed piano-rolls. They are coming as a serialized file including a dictionary with training, validation and test partitions. In this example, we are using the \"piano-midi.de\"-datset, because it is relatively small compared to the other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.normpath(r\"E:\\MusicPrediction\\Piano-midi.de.pickle\")\n",
    "dataset = load(dataset_path)\n",
    "training_set = dataset['train']\n",
    "validation_set = dataset['valid']\n",
    "test_set = dataset['test']\n",
    "print(\"Number of sequences in the training, validation and test set: {0}, {1}, {2}\".format(len(training_set), len(validation_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "We use the MultiLabelBinarizer to transform the sequences of MIDI pitches into one-hot encoded vectors. Although the piano is restricted to 88 keys, we are initializing the MultiLabelBinarizer with 128 possible pitches to stay more general. Note that this does not affect the performance critically. \n",
    "\n",
    "We can see that the sequences have different lenghts, but consist of vector with 128 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(classes=range(128))\n",
    "training_set = [mlb.fit_transform(training_set[k]) for k in range(len(training_set))]\n",
    "validation_set = [mlb.fit_transform(validation_set[k]) for k in range(len(validation_set))]\n",
    "test_set = [mlb.fit_transform(training_set[k]) for k in range(len(test_set))]\n",
    "print(\"Shape of first sequences in the training, validation and test set: {0}, {1}, {2}\".format(training_set[0].shape, validation_set[0].shape, test_set[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a basic ESN\n",
    "\n",
    "To develop an ESN model for musical note prediction, we need to tune several hyper-parameters, e.g., input_scaling, spectral_radius, bias_scaling and leaky integration.\n",
    "\n",
    "We follow the way proposed in the introductory paper of PyRCN to optimize hyper-parameters sequentially.\n",
    "\n",
    "We start to jointly optimize input_scaling and spectral_radius and therefore deactivate bias connections and leaky integration. This is our base_esn.\n",
    "\n",
    "We define the search space for input_scaling and spectral_radius. This is done using best practice and background information from the literature: The spectral radius, the largest absolute eigenvalue of the reservoir matrix, is often smaller than 1. Thus, we can search in a space between 0.0 (e.g. no recurrent connections) and 1.0 (maximum recurrent connections). It is usually recommended to tune the input_scaling factor between 0.1 and 1.0. However, as this is strongly task-dependent, we decided to slightly increase the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'input_to_node__hidden_layer_size': [50],\n",
    "    'input_to_node__input_scaling': np.linspace(start=0.1, stop=1, num=10),\n",
    "    'input_to_node__bias_scaling': [0.0],\n",
    "    'input_to_node__activation': ['identity'],\n",
    "    'input_to_node__random_state': [42],\n",
    "    'node_to_node__hidden_layer_size': [50],\n",
    "    'node_to_node__leakage': [1.0],\n",
    "    'node_to_node__spectral_radius': np.linspace(start=0.0, stop=1, num=11),\n",
    "    'node_to_node__bias_scaling': [0.0],\n",
    "    'node_to_node__activation': ['tanh'],\n",
    "    'node_to_node__random_state': [42],\n",
    "    'regressor__alpha': [1e-3],\n",
    "    'random_state': [42] }\n",
    "\n",
    "base_esn = ESNRegressor(input_to_node=InputToNode(), node_to_node=NodeToNode(), regressor=IncrementalRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize input_scaling and spectral_radius\n",
    "\n",
    "We use the ParameterGrid from scikit-learn, which converts the grid parameters defined before into a list of dictionaries for each parameter combination. \n",
    "\n",
    "We loop over each entry of the Parameter Grid, set the parameters in reg and fit our model on the training data. Afterwards, we report the MSE on the training and validation set.  \n",
    "\n",
    "    The lowest training MSE: 0.000238243207656839; parameter combination: {'input_scaling': 0.4, 'spectral_radius': 0.5}\n",
    "    The lowest validation MSE: 0.000223548432343247; parameter combination: {'input_scaling': 0.4, 'spectral_radius': 0.5}\n",
    "\n",
    "We use the best parameter combination from the validation set.\n",
    "\n",
    "As we can see in the python call, we have modified the training procedure: We use \"partial_fit\" in order to present the ESN all sequences independently from each other. The function \"partial_fit\" is part of the scikit-learn API. We have added one optional argument \"update_output_weights\". By default, it is True and thus, after feeding one sequence through the ESN, output weights are computed.\n",
    "\n",
    "However, as this is computationally expensive, we can deactivate computing output weights after each sequence by setting \"update_output_weights\" to False. Now, we simply collect sufficient statistics for the later linear regression. To finish the training process, we call finalize() after passing all sequences through the ESN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in ParameterGrid(param_grid):\n",
    "    print(params)\n",
    "    esn = clone(base_esn)\n",
    "    esn.set_params(**params)\n",
    "    for X in training_set[:-1]:\n",
    "        esn.partial_fit(X=X[:-1, :], y=X[1:, :], postpone_inverse=True)\n",
    "    X = training_set[-1]\n",
    "    esn.partial_fit(X=X[:-1, :], y=X[1:, :], postpone_inverse=False)\n",
    "    err_train = []\n",
    "    for X in training_set:\n",
    "        y_pred = esn.predict(X=X[:-1, :])\n",
    "        err_train.append(mean_squared_error(X[1:, :], y_pred))\n",
    "    err_test = []\n",
    "    for X in validation_set:\n",
    "        y_pred = esn.predict(X=X[:-1, :])\n",
    "        err_test.append(mean_squared_error(X[1:, :], y_pred))\n",
    "    print('{0}\\t{1}'.format(np.mean(err_train), np.mean(err_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update parameter of the basic ESN\n",
    "\n",
    "After optimizing input_scaling and spectral_radius, we update our basic ESN with the identified values for input_scaling and spectral_radius. \n",
    "\n",
    "For the next optimization step, we jointly optimize bias and leakage.\n",
    "\n",
    "We define the search space for bias and leakage. This is again done using best practice and background information from the literature: The bias often lies in a similar value range as the input scaling. Thus we use exactly the same search space as before. The leakage, the parameter of the leaky integration is defined in (0.0, 1.0]. Thus, we tune the leakage between 0.1 and 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'input_to_node__hidden_layer_size': [50],\n",
    "    'input_to_node__input_scaling': [0.4],\n",
    "    'input_to_node__bias_scaling': [0.0],\n",
    "    'input_to_node__activation': ['identity'],\n",
    "    'input_to_node__random_state': [42],\n",
    "    'node_to_node__hidden_layer_size': [50],\n",
    "    'node_to_node__leakage': np.linspace(start=0.1, stop=1, num=10),\n",
    "    'node_to_node__spectral_radius': 0.5,\n",
    "    'node_to_node__bias_scaling': np.linspace(start=0.0, stop=1, num=11),\n",
    "    'node_to_node__activation': ['tanh'],\n",
    "    'node_to_node__random_state': [42],\n",
    "    'regressor__alpha': [1e-3],\n",
    "    'random_state': [42] }\n",
    "\n",
    "base_esn = ESNRegressor(input_to_node=InputToNode(), node_to_node=NodeToNode(), regressor=IncrementalRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize bias and leakage\n",
    "\n",
    "The optimization workflow is exactly the same as before: We define a ParameterGrid, loop over each entry, set the parameters in reg and fit our model on the training data. Afterwards, we report the MSE on the training and validation set.  \n",
    "\n",
    "    The lowest training MSE: 0.000229618469284352; parameter combination: {'bias': 0.8, 'leakage': 0.2}\n",
    "    The lowest validation MSE: 0.000213898523704083; parameter combination: {'bias': 0.1, 'leakage': 0.2}\n",
    "\n",
    "We use the best parameter combination from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for params in ParameterGrid(param_grid):\n",
    "    print(params)\n",
    "    esn = clone(base_esn)\n",
    "    esn.set_params(**params)\n",
    "    for X in training_set[:-1]:\n",
    "        esn.partial_fit(X=X[:-1, :], y=X[1:, :], postpone_inverse=True)\n",
    "    X = training_set[-1]\n",
    "    esn.partial_fit(X=X[:-1, :], y=X[1:, :], postpone_inverse=False)\n",
    "    err_train = []\n",
    "    for X in training_set:\n",
    "        y_pred = esn.predict(X=X[:-1, :])\n",
    "        err_train.append(mean_squared_error(X[1:, :], y_pred))\n",
    "    err_test = []\n",
    "    for X in validation_set:\n",
    "        y_pred = esn.predict(X=X[:-1, :])\n",
    "        err_test.append(mean_squared_error(X[1:, :], y_pred))\n",
    "    print('{0}\\t{1}'.format(np.mean(err_train), np.mean(err_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update parameter of the basic ESN\n",
    "\n",
    "After optimizing bias and leakage, we update our basic ESN with the identified values for bias and leakage. \n",
    "\n",
    "Finally, we would quickly like to see whether the regularization parameter beta lies in the correct range.\n",
    "\n",
    "Typically, it is rather difficult to find a proper search range. Here, we use a very rough logarithmic search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'input_to_node__hidden_layer_size': [50],\n",
    "    'input_to_node__input_scaling': [0.4],\n",
    "    'input_to_node__bias_scaling': [0.0],\n",
    "    'input_to_node__activation': ['identity'],\n",
    "    'input_to_node__random_state': [42],\n",
    "    'node_to_node__hidden_layer_size': [50],\n",
    "    'node_to_node__leakage': [0.2],\n",
    "    'node_to_node__spectral_radius': 0.5,\n",
    "    'node_to_node__bias_scaling': [0.1],\n",
    "    'node_to_node__activation': ['tanh'],\n",
    "    'node_to_node__random_state': [42],\n",
    "    'regressor__alpha': [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 5e-1, 1e0],\n",
    "    'random_state': [42] }\n",
    "\n",
    "base_esn = ESNRegressor(input_to_node=InputToNode(), node_to_node=NodeToNode(), regressor=IncrementalRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize beta\n",
    "\n",
    "The optimization workflow is exactly the same as before: We define a ParameterGrid, loop over each entry, set the parameters in reg and fit our model on the training data. Afterwards, we report the MSE on the training and test set.  \n",
    "\n",
    "    The lowest training MSE: 0.00012083938686566446; parameter combination: {'beta': 5e-4}\n",
    "    The lowest validation MSE: 0.00011885985457347002; parameter combination: {'beta': 5e-3}\n",
    "\n",
    "We use the best parameter combination from the validation set, because the regularization is responsible to prevent overfitting on the training set. In a running system, of course, we should determine the regularization on a separate validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in ParameterGrid(param_grid):\n",
    "    print(params)\n",
    "    esn = clone(base_esn)\n",
    "    esn.set_params(**params)\n",
    "    for X in training_set[:-1]:\n",
    "        esn.partial_fit(X=X[:-1, :], y=X[1:, :], postpone_inverse=True)\n",
    "    X = training_set[-1]\n",
    "    esn.partial_fit(X=X[:-1, :], y=X[1:, :], postpone_inverse=False)\n",
    "    err_train = []\n",
    "    for X in training_set:\n",
    "        y_pred = esn.predict(X=X[:-1, :])\n",
    "        err_train.append(mean_squared_error(X[1:, :], y_pred))\n",
    "    err_test = []\n",
    "    for X in validation_set:\n",
    "        y_pred = esn.predict(X=X[:-1, :])\n",
    "        err_test.append(mean_squared_error(X[1:, :], y_pred))\n",
    "    print('{0}\\t{1}'.format(np.mean(err_train), np.mean(err_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update parameter of the basic ESN\n",
    "\n",
    "After optimizing beta, we update our basic ESN with the identified value for beta.\n",
    "\n",
    "Note that we have used almost the ideal value already in the beginning. Thus, the impact is rather small.\n",
    "\n",
    "Next, we want to measure the classification accuracy. To do that, we compare several reservoir sizes as well as unidirectional and bidirectional architectures.\n",
    "\n",
    "Because this is a rather small dataset, we can use rather small reservoir sizes and increase it up to 5000 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'input_to_node__hidden_layer_size': [500, 1000, 2000, 4000, 5000],\n",
    "    'input_to_node__input_scaling': [0.4],\n",
    "    'input_to_node__bias_scaling': [0.0],\n",
    "    'input_to_node__activation': ['identity'],\n",
    "    'input_to_node__random_state': [42],\n",
    "    'node_to_node__hidden_layer_size': [50],\n",
    "    'node_to_node__leakage': [0.2],\n",
    "    'node_to_node__spectral_radius': 0.5,\n",
    "    'node_to_node__bi_directional': [False, True],\n",
    "    'node_to_node__bias_scaling': [0.1],\n",
    "    'node_to_node__activation': ['tanh'],\n",
    "    'node_to_node__random_state': [42],\n",
    "    'regressor__alpha': [5e-3],\n",
    "    'random_state': [42] }\n",
    "\n",
    "base_esn = ESNRegressor(input_to_node=InputToNode(), node_to_node=NodeToNode(), regressor=IncrementalRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the ESN\n",
    "\n",
    "In the test case, we train the ESN using the entire training and validation set as seen before. Next, we compute the predicted outputs on the training, validation and test set and fix a threshold of 0.5, above a note is assumed to be predicted.\n",
    "\n",
    "We report the accuracy score for each frame in order to follow the reference paper. \n",
    "\n",
    "As can be seen, the bidirectional mode has a very strong impact on the classification result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(params)\n",
    "    esn = clone(base_esn)\n",
    "    esn.set_params(**params)\n",
    "    esn.node_to_node.hidden_layer_size = params[\"input_to_node__hidden_layer_size\"]\n",
    "    esn.finalize()\n",
    "    err_train = []\n",
    "    for X in training_set + validation_set:\n",
    "        y_pred = esn.predict(X=X[:-1, :], keep_reservoir_state=False)\n",
    "        y_pred_bin = np.asarray(y_pred > 0.1, dtype=int)\n",
    "        err_train.append(accuracy_score(y_true=X[1:, :], y_pred=y_pred_bin))\n",
    "    err_test = []\n",
    "    for X in test_set:\n",
    "        y_pred = esn.predict(X=X[:-1, :], keep_reservoir_state=False)\n",
    "        print(np.sum(y_pred, axis=0))\n",
    "        y_pred_bin = np.asarray(y_pred > 0.1, dtype=int)\n",
    "        err_test.append(accuracy_score(y_true=X[1:, :], y_pred=y_pred_bin))\n",
    "    print('{0}\\t{1}'.format(np.mean(err_train), np.mean(err_test)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
