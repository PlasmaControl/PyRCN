{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of musical notes\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook adapts one reference experiment for note prediction using ESNs from ([https://arxiv.org/abs/1812.11527](https://arxiv.org/abs/1812.11527)) to PyRCN and shows that introducing bidirectional ESNs significantly improves the results in terms of Accuracy, already for rather small networks.\n",
    "\n",
    "The tutorial is based on numpy, scikit-learn, joblib and PyRCN. We are using the ESNRegressor, because we further process the outputs of the ESN. Note that the same can also be done using the ESNClassifier. Then, during prediction, we simply call \"predict_proba\".\n",
    "\n",
    "This tutorial requires the Python modules numpy, scikit-learn, matplotlib and pyrcn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from joblib import load\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import ParameterGrid, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from scipy.stats import uniform\n",
    "\n",
    "from pyrcn.echo_state_network import ESNClassifier\n",
    "from pyrcn.model_selection import SequentialSearchCV\n",
    "from pyrcn.metrics import mean_squared_error, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "The datasets are online available at ([http://www-etud.iro.umontreal.ca/~boulanni/icml2012](http://www-etud.iro.umontreal.ca/~boulanni/icml2012)). In this notebook, we use the pre-processed piano-rolls. They are coming as a serialized file including a dictionary with training, validation and test partitions. In this example, we are using the \"piano-midi.de\"-datset, because it is relatively small compared to the other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.normpath(r\"E:\\MusicPrediction\\Piano-midi.de.pickle\")\n",
    "dataset = load(dataset_path)\n",
    "X_train = np.empty(shape=(len(dataset['train']) + len(dataset['valid']), ),\n",
    "                   dtype=object)\n",
    "y_train = np.empty(shape=(len(dataset['train']) + len(dataset['valid']), ),\n",
    "                   dtype=object)\n",
    "\n",
    "X_test = np.empty(shape=(len(dataset['test']), ), dtype=object)\n",
    "y_test = np.empty(shape=(len(dataset['test']), ), dtype=object)\n",
    "print(\"Number of sequences in the training and test set: {0}, {1}\"\n",
    "      .format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "We use the MultiLabelBinarizer to transform the sequences of MIDI pitches into one-hot encoded vectors. Although the piano is restricted to 88 keys, we are initializing the MultiLabelBinarizer with 128 possible pitches to stay more general. Note that this does not affect the performance critically. \n",
    "\n",
    "We can see that the sequences have different lenghts, but consist of vector with 128 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(classes=range(128))\n",
    "for k, X in enumerate(dataset['train'] + dataset['valid']):\n",
    "    X_train[k] = mlb.fit_transform(X[:-1])\n",
    "    y_train[k] = mlb.fit_transform(X[1:])\n",
    "for k, X in enumerate(dataset['test']):\n",
    "    X_test[k] = mlb.fit_transform(X[:-1])\n",
    "    y_test[k] = mlb.fit_transform(X[1:])\n",
    "print(\"Shape of first sequences in the training and test set: {0}, {1}\"\n",
    "      .format(X_train[0].shape, X_test[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a basic ESN\n",
    "\n",
    "To develop an ESN model for musical note prediction, we need to tune several hyper-parameters, e.g., input_scaling, spectral_radius, bias_scaling and leaky integration.\n",
    "\n",
    "We follow the way proposed in the introductory paper of PyRCN to optimize hyper-parameters sequentially.\n",
    "\n",
    "We start to jointly optimize input_scaling and spectral_radius and therefore deactivate bias connections and leaky integration. This is our base_esn.\n",
    "\n",
    "We define the search space for input_scaling and spectral_radius. This is done using best practice and background information from the literature: The spectral radius, the largest absolute eigenvalue of the reservoir matrix, is often smaller than 1. Thus, we can search in a space between 0.0 (e.g. no recurrent connections) and 1.0 (maximum recurrent connections). It is usually recommended to tune the input_scaling factor between 0.1 and 1.0. However, as this is strongly task-dependent, we decided to slightly increase the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initially_fixed_params = {'hidden_layer_size': 50,\n",
    "                          'input_activation': 'identity',\n",
    "                          'k_in': 10,\n",
    "                          'input_scaling': 0.4,\n",
    "                          'bias_scaling': 0.0,\n",
    "                          'spectral_radius': 0.0,\n",
    "                          'reservoir_activation': 'tanh',\n",
    "                          'leakage': 1.0,\n",
    "                          'bidirectional': False,\n",
    "                          'k_rec': 10,\n",
    "                          'alpha': 1e-3,\n",
    "                          'random_state': 42}\n",
    "\n",
    "step1_esn_params = {'input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'spectral_radius': uniform(loc=0, scale=2)}\n",
    "step2_esn_params = {'leakage': loguniform(1e-5, 1e0)}\n",
    "step3_esn_params = {'bias_scaling': np.linspace(0.0, 1.0, 11)}\n",
    "step4_esn_params = {'alpha': loguniform(1e-5, 1e1)}\n",
    "\n",
    "kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1,\n",
    "                'scoring': make_scorer(mean_squared_error, greater_is_better=False,\n",
    "                                       needs_proba=True)}\n",
    "kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1,\n",
    "                'scoring': make_scorer(mean_squared_error, greater_is_better=False,\n",
    "                                       needs_proba=True)}\n",
    "kwargs_step3 = {'verbose': 1, 'n_jobs': -1,\n",
    "                'scoring': make_scorer(mean_squared_error,greater_is_better=False,\n",
    "                                       needs_proba=True)}\n",
    "kwargs_step4 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1,\n",
    "                'scoring': make_scorer(mean_squared_error, greater_is_better=False,\n",
    "                                       needs_proba=True)}\n",
    "\n",
    "# The searches are defined similarly to the steps of a sklearn.pipeline.Pipeline:\n",
    "searches = [('step1', RandomizedSearchCV, step1_esn_params, kwargs_step1),\n",
    "            ('step2', RandomizedSearchCV, step2_esn_params, kwargs_step2),\n",
    "            ('step3', GridSearchCV, step3_esn_params, kwargs_step3),\n",
    "            ('step4', RandomizedSearchCV, step4_esn_params, kwargs_step4)]\n",
    "\n",
    "base_esn = ESNClassifier(**initially_fixed_params)\n",
    "sequential_search = \\\n",
    "    SequentialSearchCV(base_esn, searches=searches).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the ESN\n",
    "\n",
    "In the test case, we train the ESN using the entire training and validation set as seen before. Next, we compute the predicted outputs on the training, validation and test set and fix a threshold of 0.5, above a note is assumed to be predicted.\n",
    "\n",
    "We report the accuracy score for each frame in order to follow the reference paper. \n",
    "\n",
    "As can be seen, the bidirectional mode has a very strong impact on the classification result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'hidden_layer_size': [500, 1000, 2000, 4000, 5000]}\n",
    "base_esn = sequential_search.best_estimator_\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(params)\n",
    "    esn = clone(base_esn).set_params(**params)\n",
    "    esn.fit(X_train, y_train)\n",
    "    training_score = accuracy_score(y_train, esn.predict(X_train))\n",
    "    test_score = accuracy_score(y_test, esn.predict(X_test))\n",
    "    print('{0}\\t{1}'.format(training_score, test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
