{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster-based Input Weight Initialization for Echo State Networks\n",
    "\n",
    "This notebook aims to be the supplemental material for the corresponding journal article.\n",
    "\n",
    "We aim to pre-train the input weight matrix of ESNs using the K-Means algorithm since passing features to the non-linear reservoir of ESNs is closely related to compute the dot product between two vectors.\n",
    "\n",
    "We use various datasets from https://github.com/FilippoMB/Time-series-classification-and-clustering-with-Reservoir-Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, ParameterGrid, cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from scipy.stats import uniform\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from joblib import dump, load\n",
    "from pyrcn.echo_state_network import ESNClassifier\n",
    "from pyrcn.base.blocks import PredefinedWeightsInputToNode, NodeToNode\n",
    "from pyrcn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error\n",
    "from pyrcn.model_selection import SequentialSearchCV\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "%matplotlib inline\n",
    "#Options\n",
    "plt.rc('image', cmap='RdBu')\n",
    "plt.rc('font', family='serif', serif='Times')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('xtick', labelsize=8)\n",
    "plt.rc('ytick', labelsize=8)\n",
    "plt.rc('axes', labelsize=8)\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png', 'pdf')\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib import ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Select a dataset\n",
    "\n",
    "In this study, we have worked with the [datasets](https://mega.nz/#!aZkBwYDa!JZb99GQoUn4EoJYceCK3Ihe04hhYZYuIWn018gcQM8k)[^1]. \n",
    "\n",
    "For more information, we refer to the websites of the distinct datasets:\n",
    "- [Spoken Arabic Digits](https://archive.ics.uci.edu/ml/datasets/Spoken+Arabic+Digit)\n",
    "- [Australian Sign Language signs (High Quality) Data Set](https://archive.ics.uci.edu/ml/datasets/Australian+Sign+Language+signs+(High+Quality))\n",
    "- [Character Trajectories Data Set](http://archive.ics.uci.edu/ml/datasets/Character+Trajectories)\n",
    "- [ChlorineConcentration: Chlorine concentration data set](https://rdrr.io/github/moviedo5/fda.tsc/man/ChlorineConcentration.html)\n",
    "- [CMU Graphics Lab Motion Capture Database](http://mocap.cs.cmu.edu/)\n",
    "- ECG Dataset\n",
    "- Japanese Vowels Data Set\n",
    "- [Kicks vs. Punch Dataset](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7813879/)\n",
    "- [Libras Movement Data Set](https://archive.ics.uci.edu/ml/datasets/Libras+Movement)\n",
    "- NetFlow dataset\n",
    "- PEMS-SF Data Set\n",
    "- DistalPhalanxTW Data Set\n",
    "- [Robot Execution Failures Data Set](http://archive.ics.uci.edu/ml/datasets/Robot+Execution+Failures)\n",
    "- [SwedishLeaf Data Set](http://www.timeseriesclassification.com/description.php?Dataset=SwedishLeaf)\n",
    "- [uWave Data Set](https://www.yecl.org/publications/liu09percom.pdf)\n",
    "- [Wafer Data Set](http://www.timeseriesclassification.com/description.php?Dataset=Wafer)\n",
    "- [BasicMotions](http://www.timeseriesclassification.com/description.php?Dataset=BasicMotions)\n",
    "\n",
    "[^1]: https://github.com/FilippoMB/Time-series-classification-and-clustering-with-Reservoir-Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = list(Path(r\"E:\\multivariate_time_series_dataset\\numpy\").rglob(\"*.npz\"))\n",
    "fileMenu = widgets.Dropdown(options=filelist, value=None, description='Choose Dataset:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_dataset(fname):\n",
    "    if fname is not None:\n",
    "        dataset = np.load(str(fname))\n",
    "\n",
    "        X_train = np.empty(shape=(dataset['X'].shape[0], ), dtype=object)\n",
    "        y_train = np.empty(shape=(dataset['X'].shape[0], ), dtype=object)\n",
    "        X_test = np.empty(shape=(dataset['Xte'].shape[0], ), dtype=object)\n",
    "        y_test = np.empty(shape=(dataset['Xte'].shape[0], ), dtype=object)\n",
    "        for k, (X, y) in enumerate(zip(dataset['X'], dataset['Y'])):\n",
    "            X_train[k] = X[X.sum(axis=1)!=0, :]  # Sequences are zeropadded -> should we remove zeros? if not, X_train[k] = X\n",
    "            y_train[k] = np.argwhere(y).ravel()\n",
    "        scaler = StandardScaler().fit(np.concatenate(X_train))\n",
    "        for k, X in enumerate(X_train):\n",
    "            X_train[k] = scaler.transform(X=X)  # Sequences are zeropadded -> should we remove zeros? if not, X_train[k] = X\n",
    "        X_train, y_train = shuffle(X_train, y_train, random_state=0)\n",
    "        for k, (X, y) in enumerate(zip(dataset['Xte'], dataset['Yte'])):\n",
    "            X_test[k] = scaler.transform(X=X[X.sum(axis=1)!=0, :])  # Sequences are zeropadded -> should we remove zeros? if not, X_train[k] = X\n",
    "            y_test[k] = np.argwhere(y).ravel()\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    return None\n",
    "\n",
    "data_widget = interactive(load_and_preprocess_dataset, {\"manual\": True}, fname=fileMenu)\n",
    "data_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = data_widget.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit random ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initially_fixed_params = {'hidden_layer_size': 50,\n",
    "                          'input_activation': 'identity',\n",
    "                          'k_in': 10,\n",
    "                          'input_scaling': 0.4,\n",
    "                          'bias_scaling': 0.0,\n",
    "                          'spectral_radius': 0.0,\n",
    "                          'reservoir_activation': 'tanh',\n",
    "                          'leakage': 0.1,\n",
    "                          'bidirectional': False,\n",
    "                          'k_rec': 10,\n",
    "                          'alpha': 1e-3,\n",
    "                          'random_state': 42,\n",
    "                          'decision_strategy': \"winner_takes_all\"}\n",
    "\n",
    "step1_esn_params = {'input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'spectral_radius': uniform(loc=0, scale=2)}\n",
    "\n",
    "step2_esn_params = {'leakage': loguniform(1e-5, 1e0)}\n",
    "step3_esn_params = {'bias_scaling': np.linspace(0.0, 1.0, 11)}\n",
    "step4_esn_params = {'alpha': loguniform(1e-5, 1e1)}\n",
    "\n",
    "kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(mean_squared_error, greater_is_better=False, needs_proba=True)}\n",
    "kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(mean_squared_error, greater_is_better=False, needs_proba=True)}\n",
    "kwargs_step3 = {'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(mean_squared_error, greater_is_better=False, needs_proba=True)}\n",
    "kwargs_step4 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(mean_squared_error, greater_is_better=False, needs_proba=True)}\n",
    "\n",
    "# The searches are defined similarly to the steps of a sklearn.pipeline.Pipeline:\n",
    "searches = [('step1', RandomizedSearchCV, step1_esn_params, kwargs_step1),\n",
    "            ('step2', RandomizedSearchCV, step2_esn_params, kwargs_step2),\n",
    "            ('step3', GridSearchCV, step3_esn_params, kwargs_step3),\n",
    "            ('step4', RandomizedSearchCV, step4_esn_params, kwargs_step4)]\n",
    "\n",
    "base_esn = ESNClassifier(**initially_fixed_params)\n",
    "try:\n",
    "    sequential_search = load(\"../multidataset/sequential_search_\" + os.path.splitext(os.path.basename(fileMenu.label))[0] + \".joblib\")\n",
    "except FileNotFoundError:\n",
    "    sequential_search = SequentialSearchCV(base_esn, searches=searches).fit(X_train, y_train)\n",
    "    dump(sequential_search, \"../multidataset/sequential_search_\" + os.path.splitext(os.path.basename(fileMenu.label))[0] + \".joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_esn = clone(sequential_search.best_estimator_).set_params(**{\"hidden_layer_size\": 200})\n",
    "search = RandomizedSearchCV(estimator=base_esn, param_distributions=step4_esn_params, **kwargs_step4).fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sequential_search.all_best_params_)\n",
    "print(sequential_search.all_best_score_)\n",
    "base_esn = clone(sequential_search.best_estimator_.get_params())\n",
    "base_esn.set_params(**search.best_params)\n",
    "\n",
    "param_grid = {'hidden_layer_size': [50, 100, 200, 400, 800, 1600],\n",
    "              'random_state': range(1, 11)}\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    t1 = time.time()\n",
    "    esn = clone(base_esn).set_params(**params).fit(X=X_train, y=y_train, n_jobs=8)\n",
    "    t2 = time.time()\n",
    "    score = accuracy_score(y_test, esn.predict(X_test))\n",
    "    print(\"ESN with params {0} achieved score of {1} and was trained in {2} seconds.\".format(params, score, t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit KM-ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = MiniBatchKMeans(n_clusters=50, n_init=200, reassignment_ratio=0, max_no_improvement=50, init='k-means++', verbose=2, random_state=0)\n",
    "kmeans.fit(X=np.concatenate(np.concatenate((X_train, X_test))))\n",
    "w_in = np.divide(kmeans.cluster_centers_, np.linalg.norm(kmeans.cluster_centers_, axis=1)[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initially_fixed_params = {'hidden_layer_size': 50,\n",
    "                          'input_activation': 'identity',\n",
    "                          'k_in': 10,\n",
    "                          'input_scaling': 0.4,\n",
    "                          'bias_scaling': 0.0,\n",
    "                          'spectral_radius': 0.0,\n",
    "                          'reservoir_activation': 'tanh',\n",
    "                          'leakage': 0.1,\n",
    "                          'bidirectional': False,\n",
    "                          'k_rec': 10,\n",
    "                          'alpha': 1e-3,\n",
    "                          'random_state': 42,\n",
    "                          'decision_strategy': \"winner_takes_all\"}\n",
    "\n",
    "step1_esn_params = {'input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'spectral_radius': uniform(loc=0, scale=2)}\n",
    "\n",
    "step2_esn_params = {'leakage': loguniform(1e-5, 1e0)}\n",
    "step3_esn_params = {'bias_scaling': np.linspace(0.0, 1.0, 11)}\n",
    "step4_esn_params = {'alpha': loguniform(1e-5, 1e1)}\n",
    "\n",
    "kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(mean_squared_error, greater_is_better=False, needs_proba=True)}\n",
    "kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(mean_squared_error, greater_is_better=False, needs_proba=True)}\n",
    "kwargs_step3 = {'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(mean_squared_error, greater_is_better=False, needs_proba=True)}\n",
    "kwargs_step4 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(mean_squared_error, greater_is_better=False, needs_proba=True)}\n",
    "\n",
    "# The searches are defined similarly to the steps of a sklearn.pipeline.Pipeline:\n",
    "searches = [('step1', RandomizedSearchCV, step1_esn_params, kwargs_step1),\n",
    "            ('step2', RandomizedSearchCV, step2_esn_params, kwargs_step2),\n",
    "            ('step3', GridSearchCV, step3_esn_params, kwargs_step3),\n",
    "            ('step4', RandomizedSearchCV, step4_esn_params, kwargs_step4)]\n",
    "\n",
    "base_km_esn = ESNClassifier(input_to_node=PredefinedWeightsInputToNode(predefined_input_weights=w_in.T),\n",
    "                            **initially_fixed_params)\n",
    "\n",
    "try:\n",
    "    sequential_search = load(\"../multidataset/sequential_search_\" + os.path.splitext(os.path.basename(fileMenu.label))[0] + \"_km.joblib\")\n",
    "except FileNotFoundError:\n",
    "    sequential_search = SequentialSearchCV(base_km_esn, searches=searches).fit(X_train, y_train)\n",
    "    dump(sequential_search, \"../multidataset/sequential_search_\" + os.path.splitext(os.path.basename(fileMenu.label))[0] + \"_km.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_params = sequential_search.best_estimator_.get_params()\n",
    "constant_params.pop('hidden_layer_size')\n",
    "constant_params.pop('random_state')\n",
    "constant_params.pop('predefined_input_weights')\n",
    "base_esn = ESNClassifier(**constant_params)\n",
    "\n",
    "kmeans = MiniBatchKMeans(n_clusters=200, n_init=200, reassignment_ratio=0, max_no_improvement=50, init='k-means++', verbose=0, random_state=0)\n",
    "kmeans.fit(X=np.concatenate(np.concatenate((X_train, X_test))))\n",
    "w_in = np.divide(kmeans.cluster_centers_, np.linalg.norm(kmeans.cluster_centers_, axis=1)[:, None])\n",
    "base_esn.input_to_node = PredefinedWeightsInputToNode(predefined_input_weights=w_in.T)\n",
    "base_esn.set_params(**{\"hidden_layer_size\": 200}, **constant_params)\n",
    "search = RandomizedSearchCV(estimator=base_esn, param_distributions=step4_esn_params, **kwargs_step4).fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sequential_search.all_best_params_)\n",
    "print(sequential_search.all_best_score_)\n",
    "\n",
    "constant_params = sequential_search.best_estimator_.get_params()\n",
    "constant_params.pop('hidden_layer_size')\n",
    "constant_params.pop('random_state')\n",
    "constant_params.pop('predefined_input_weights')\n",
    "base_esn = ESNClassifier(**constant_params)\n",
    "base_esn.set_params(**search.best_params_)\n",
    "\n",
    "param_grid = {'hidden_layer_size': [50, 100, 200, 400, 800, 1600],\n",
    "              'random_state': range(1, 11)}\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    kmeans = MiniBatchKMeans(n_clusters=params['hidden_layer_size'], n_init=200, reassignment_ratio=0, max_no_improvement=50, init='k-means++', verbose=0, random_state=params['random_state'])\n",
    "    t1 = time.time()\n",
    "    kmeans.fit(X=np.concatenate(np.concatenate((X_train, X_test))))\n",
    "    w_in = np.divide(kmeans.cluster_centers_, np.linalg.norm(kmeans.cluster_centers_, axis=1)[:, None])\n",
    "    t2 = time.time()\n",
    "    km_esn = clone(base_esn)\n",
    "    km_esn.input_to_node = PredefinedWeightsInputToNode(predefined_input_weights=w_in.T)\n",
    "    km_esn.set_params(**constant_params, **params)\n",
    "    km_esn.fit(X=X_train, y=y_train, n_jobs=8)\n",
    "    score = accuracy_score(y_test, km_esn.predict(X_test))\n",
    "    print(\"KM-ESN with params {0} achieved score of {1} and was trained in {2} seconds.\".format(params, score, t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_search = load(\"../multidataset/sequential_search_chlo_km.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = np.random.randint(0, 800, 50)\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax = sns.histplot(data=w_in, stat=\"count\", legend=False)\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.imshow(sequential_search.best_estimator_.input_to_node.input_weights.todense()[:, idx])\n",
    "plt.colorbar()\n",
    "# plt.savefig('KM_ESN_Input_Weight_Hist_CHLO.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step1\"])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = sns.scatterplot(x=\"param_spectral_radius\", y=\"param_input_scaling\", hue=\"mean_test_score\", palette='RdBu', data=df)\n",
    "plt.xlabel(\"Spectral Radius\")\n",
    "plt.ylabel(\"Input Scaling\")\n",
    "\n",
    "norm = plt.Normalize(0.97, 1.0)\n",
    "sm = plt.cm.ScalarMappable(cmap=\"RdBu\", norm=norm)\n",
    "sm.set_array([])\n",
    "plt.xlim((0, 2.05))\n",
    "plt.ylim((0, 1.05))\n",
    "\n",
    "# Remove the legend and add a colorbar\n",
    "ax.get_legend().remove()\n",
    "ax.figure.colorbar(sm)\n",
    "fig.set_size_inches(4, 2.5)\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.yaxis.set_major_locator(tick_locator)\n",
    "ax.xaxis.set_major_locator(tick_locator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step2\"])\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax = sns.lineplot(data=df, x=\"param_leakage\", y=\"mean_test_score\")\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel(\"Leakage\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlim((1e-5, 1e0))\n",
    "tick_locator = ticker.MaxNLocator(10)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.4f'))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step3\"])\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax = sns.lineplot(data=df, x=\"param_bias_scaling\", y=\"mean_test_score\")\n",
    "plt.xlabel(\"Bias Scaling\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlim((0, 1))\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.5f'))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step4\"])\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax = sns.lineplot(data=df, x=\"param_alpha\", y=\"mean_test_score\")\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlim((1e-5, 1e0))\n",
    "tick_locator = ticker.MaxNLocator(20)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.5f'))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
