{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38932301-52a2-43c9-bb9c-08b55d9e600b",
   "metadata": {},
   "source": [
    "# Building blocks of Reservoir Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f135eaa0-ee54-4d6b-bd75-738b881ec1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "# Generate a toy dataset\n",
    "U, y = make_blobs(n_samples=100, n_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e2a74c-9f10-4ee6-a111-fca4d7edeb34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Input-to-Node\n",
    "\n",
    "The \"Input-to-Node\" component describes the connections from the input features to the reservoir and the activation functions of the reservoir neurons. Normally, the input weight matrix $\\mathbf{W}^{\\mathrm{in}}$ has the dimension of $N^{\\mathrm{res}}\\times N^{\\mathrm{in}}$, where $N^{\\mathrm{res}}$ and $N^{\\mathrm{in}}$ are the size of the reservoir and dimension of the input feature vector $\\mathbf{u}[n]$ with the time index $n$, respectively. With\n",
    "\n",
    "\\begin{align}\n",
    "    \\label{eq:InputToNode}\n",
    "    \\mathbf{r}'[n] = f'(\\mathbf{W}^{\\mathrm{in}}\\mathbf{u}[n] + \\mathbf{w}^{\\mathrm{bi}}) \\text{ , }\n",
    "\\end{align}\n",
    "\n",
    "we can describe the non-linear projection of the input features $\\mathbf{u}[n]$ into the high-dimensional reservoir space $\\mathbf{r}'[n]$ via the non-linear input activation function $f'(\\cdot)$.\n",
    "\n",
    "The values inside the input weight matrix are usually initialized randomly from a uniform distribution on the interval $[-1, 1]$ and are afterwards scaled using the input scaling factor $\\alpha_{\\mathrm{u}}$. Since in case of a high dimensional input feature space and/or large reservoir sizes $N^{\\mathrm{res}}$, this leads to a huge input weight matrix and expensive computations to feed the feature vectors into the reservoir, it was shown that it is sufficient to have only a very small number of connections from the input nodes to the nodes inside the reservoir. Each node of the reservoir may therefore be connected to only $K^{\\mathrm{in}}$ ($\\ll N^{\\mathrm{in}}$) randomly selected input entries. This makes $\\mathbf{W}^{\\mathrm{in}}$ typically very sparse and feeding the feature vectors into the reservoir potentially more efficient.\n",
    "\n",
    "The bias weights $\\mathbf{w}^{\\mathrm{bi}}$ with dimension $N^{\\mathrm{res}}$ are typically initialized by fixed random values from a uniform distribution between $\\pm 1$ and multiplied by the hyper-parameter $\\alpha_{\\mathrm{bi}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cceb89-b9dc-4d78-8eff-9e5ae39cb143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrcn.base.blocks import InputToNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffacc49-0f71-4b0b-9bc3-0c13a8a3088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#      _ _ _ _ _ _ _ _\n",
    "#     |               |\n",
    "# ----| Input-to-Node |------\n",
    "# u[n]|_ _ _ _ _ _ _ _|r'[n]\n",
    "# U                    R_i2n\n",
    "\n",
    "input_to_node = InputToNode(hidden_layer_size=50,\n",
    "                            k_in=5, input_activation=\"tanh\",\n",
    "                            input_scaling=1.0, bias_scaling=0.1)\n",
    "\n",
    "R_i2n = input_to_node.fit_transform(U)\n",
    "print(U.shape, R_i2n.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f2ea5a-492a-4745-bf82-65bfb2e5534e",
   "metadata": {},
   "source": [
    "## Node-to-Node\n",
    "\n",
    "The \"Node-to-Node\" component describes the connections inside the reservoir. The output of \"Input-to-Node\" $\\mathbf{r}'[n]$ together with the output of \"Node-to-Node\" from the previous time step $\\mathbf{r}[n-1]$ are used to compute the new output of \"Node-to-Node\" $\\mathbf{r}[n]$ using\n",
    "\n",
    "\\begin{align}\n",
    "   \\label{eq:NodeToNode}\n",
    "   \\mathbf{r}[n] = (1-\\lambda)\\mathbf{r}[n-1] + \\lambda f(\\mathbf{r}'[n] + \\mathbf{W}^{\\mathrm{res}}\\mathbf{r}[n-1])\\text{ , }\n",
    "\\end{align}\n",
    "\n",
    "which is a leaky integration of the time-dependent reservoir states $\\mathbf{r}[n]$. $f(\\cdot)$ acts as the non-linear reservoir activation functions of the neurons in \"Node-to-Node\". The leaky integration is equivalent to a first-order lowpass filter. Depending  on the leakage $\\lambda \\in (0, 1]$, the reservoir states are globally smoothed.\n",
    "\n",
    "The reservoir weight matrix $\\mathbf{W}^{\\mathrm{res}}$ is a square matrix of the size $N^{\\mathrm{res}}$. These weights are typically initialized from a standard normal distribution. The Echo State Property (ESP) requires that the states of all reservoir neurons need to decay in a finite time for a finite input pattern. In order to fulfill the ESP, the reservoir weight matrix is typically normalized by its largest absolute eigenvalue and rescaled to a spectral radius $\\rho$, because it was shown that the ESP holds as long as $\\rho \\le 1$. The spectral radius and the leakage together shape the temporal memory of the reservoir. Similar as for \"Input-to-Node\", the reservoir weight matrix gets huge in case of large reservoir sizes $N^{\\mathrm{res}}$, it can be sufficient to only connect each node in the reservoir only to $K^{\\mathrm{rec}}$ ($\\ll N^{\\mathrm{res}}$) randomly selected other nodes in the reservoir, and to set the remaining weights to zero.\n",
    "\n",
    "To incorporate some information from the future inputs, bidirectional RCNs have been introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6732b6-f733-4b52-9a9d-213244bedeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrcn.base.blocks import NodeToNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1353f360-8f14-40c3-a39b-af8d232e01ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#      _ _ _ _ _ _ _ _        _ _ _ _ _ _ _\n",
    "#     |               |      |              |\n",
    "# ----| Input-to-Node |------| Node-to-Node |------\n",
    "# u[n]|_ _ _ _ _ _ _ _|r'[n] |_ _ _ _ _ _ _ |r[n]\n",
    "# U                    R_i2n                 R_n2n\n",
    "\n",
    "# Initialize, fit and apply NodeToNode\n",
    "node_to_node = NodeToNode(hidden_layer_size=50,\n",
    "                          reservoir_activation=\"tanh\",\n",
    "                          spectral_radius=1.0, leakage=0.9,\n",
    "                          bidirectional=False)\n",
    "R_n2n = node_to_node.fit_transform(R_i2n)\n",
    "print(U.shape, R_n2n.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6a398e-f7fd-4aee-80d2-8e0c1bbfd9b9",
   "metadata": {},
   "source": [
    "The \"Node-to-Output\" component is the mapping of the reservoir state $\\mathbf{r}[n]$ to the output $\\mathbf{y}[n]$ of the network. In conventional RCNs, this mapping is trained using (regularized) linear regression. To that end, all reservoir states $\\mathbf{r}[n]$ are concatenated into the reservoir state collection matrix $\\mathbf{R}$. As linear regression usually contains an intercept term, every reservoir state $\\mathbf{r}[n]$ is expanded by a constant of 1. All desired outputs $\\mathbf{d}[n]$ are collected into the desired output collection matrix $\\mathbf{D}$. Then, the mapping matrix $\\mathbf{W}^{\\mathrm{out}}$ can be computed using\n",
    "\n",
    "\\begin{align}\n",
    "    \\label{eq:linearRegression}\n",
    "    \\mathbf{W}^{\\mathrm{out}} =\\left(\\mathbf{R}\\mathbf{R}^{\\mathrm{T}} + \\epsilon\\mathbf{I}\\right)^{-1}(\\mathbf{D}\\mathbf{R}^{\\mathrm{T}}) \\text{,}\n",
    "\\end{align}\n",
    "\n",
    "where $\\epsilon$ is a regularization parameter.\n",
    "\n",
    "The size of the output weight matrix $N^{\\mathrm{out}}\\times (N^{\\mathrm{res}} + 1)$ or $N^{\\mathrm{out}}\\times (2 \\times N^{\\mathrm{res}} + 1)$ in case of a bidirectional \"Node-to-Node\" determines the total number of free parameters to be trained in the neural network. \n",
    "\n",
    "After training, the output $\\mathbf{y}[n]$ can be computed using Equation \n",
    "\n",
    "\\begin{align}\n",
    "\\label{eq:readout}\n",
    "\\mathbf{y}[n] = \\mathbf{W}^{\\mathrm{out}}\\mathbf{r}[n] \\text{ . }\n",
    "\\end{align}\n",
    "\n",
    "Note that, in general, other training methodologies could be used to compute output weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915a0f1-d5e9-4463-a81c-6cfb9fdc44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fce3042-4e40-4f28-b240-d9b7300ffad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#       _ _ _ _ _ _ _       _ _ _ _ _ _ _        _ _ _ _ _ _ _        \n",
    "#     |              |     |             |     |               |       \n",
    "# ----|Input-to-Node |-----|Node-to-Node |-----|Node-to-Output |\n",
    "# u[n]| _ _ _ _ _ _ _|r'[n]|_ _ _ _ _ _ _|r[n] | _ _ _ _ _ _ _ |\n",
    "# U                   R_i2n               R_n2n        |\n",
    "#                                                      |\n",
    "#                                                 y[n] | y_pred\n",
    "\n",
    "# Initialize, fit and apply NodeToOutput\n",
    "y_pred = Ridge().fit(R_n2n, y).predict(R_n2n)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd1fc50-0055-411c-a613-d7a48aee027f",
   "metadata": {},
   "source": [
    "## Predict the Mackey-Glass equation\n",
    "\n",
    "Set up and train vanilla RCNs for predicting the Mackey-Glass time series with the same settings as used to introduce ESNs. The minimum working example shows the simplicity of implementing a model with PyRCN and the inter-operability with scikit-learn; it needs only four lines of code to load the Mackey-Glass dataset that is part of PyRCN and only two lines to fit the different RCN models, respectively. Instead of the default incremental regression, we have customized the ```ELMRegressor()``` by using ```Ridge``` from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135841e7-3210-488d-8f6c-3c53fa7a4766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge as skRidge\n",
    "from pyrcn.echo_state_network import ESNRegressor\n",
    "from pyrcn.extreme_learning_machine import ELMRegressor\n",
    "from pyrcn.datasets import mackey_glass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4af78bd-a3e1-420e-935a-23ba88127acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "X, y = mackey_glass(n_timesteps=5000)\n",
    "# Define Train/Test lengths\n",
    "trainLen = 1900\n",
    "X_train, y_train = X[:trainLen], y[:trainLen]\n",
    "X_test, y_test = X[trainLen:], y[trainLen:]\n",
    "\n",
    "# Initialize and train an ELMRegressor and an ESNRegressor\n",
    "esn = ESNRegressor().fit(X=X_train.reshape(-1, 1), y=y_train)\n",
    "elm = ELMRegressor(regressor=skRidge()).fit(X=X_train.reshape(-1, 1), y=y_train)\n",
    "\n",
    "print(\"Fitted models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aa87dd-84c7-400b-9004-f4864ffb43ba",
   "metadata": {},
   "source": [
    "# Build Reservoir Computing Networks with PyRCN\n",
    "\n",
    "By combining the building blocks introduced above, a vast number of different RCNs can be constructed. In this section, we build two important variants of RCNs, namely ELMs and ESNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b174e488-7119-48db-a36a-8c37510ff5de",
   "metadata": {},
   "source": [
    "# Extreme Learning Machines\n",
    "\n",
    "The vanilla ELM as a single-layer feedforward network consists of an \"Input-to-Node\" and a \"Node-to-Output\" module and is trained in two steps:\n",
    "\n",
    "1. Compute the high-dimensional reservoir states $\\mathbf{R}'$, which is the collection of reservoir states $\\mathbf{r}'[n]$.\n",
    "2. Compute the output weights $\\mathbf{W}^{\\mathrm{out}}$ with $\\mathbf{R}'$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7ebffd-b29d-436c-8dbf-d4976537f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, y = make_blobs(n_samples=100, n_features=10)\n",
    "from pyrcn.extreme_learning_machine import ELMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1365640-0b9b-42c3-8b12-80de7c6cb853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla ELM for regression tasks with input_scaling\n",
    "#       _ _ _ _ _ _ _        _ _ _ _ _ _ _        \n",
    "#     |              |     |               |       \n",
    "# ----|Input-to-Node |-----|Node-to-Output |------\n",
    "# u[n]| _ _ _ _ _ _ _|r'[n]| _ _ _ _ _ _ _ |y[n]\n",
    "#                                           y_pred\n",
    "# \n",
    "vanilla_elm = ELMRegressor(input_scaling=0.9)\n",
    "vanilla_elm.fit(U, y)\n",
    "print(vanilla_elm.predict(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c4803c-0bfa-4de1-8961-65b7daf5c248",
   "metadata": {},
   "source": [
    "Example of how to construct an ELM with a BIP \"Input-to-Node\" ELMs with PyRCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32940275-c102-4502-a94d-5cdb3a5a855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrcn.base.blocks import BatchIntrinsicPlasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5186f53f-37e1-4d4c-8497-37bc01bf5179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom ELM with BatchIntrinsicPlasticity\n",
    "#       _ _ _ _ _ _ _        _ _ _ _ _ _ _        \n",
    "#     |              |     |               |       \n",
    "# ----|     BIP      |-----|Node-to-Output |------\n",
    "# u[n]| _ _ _ _ _ _ _|r'[n]| _ _ _ _ _ _ _ |y[n]\n",
    "#                                           y_pred\n",
    "# \n",
    "bip_elm = ELMRegressor(input_to_node=BatchIntrinsicPlasticity(),\n",
    "                       regressor=Ridge(alpha=1e-5))\n",
    "\n",
    "bip_elm.fit(U, y)\n",
    "print(bip_elm.predict(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1d0702-8532-4c9b-bd47-03c4c4001aa0",
   "metadata": {},
   "source": [
    "Hierarchical or Ensemble ELMs can then be built using multiple \"Input-to-Node\" modules in parallel or in a cascade. This is possible when using using scikit-learn's ```sklearn.pipeline.Pipeline``` (cascading) or ```sklearn.pipeline.FeatureUnion``` (ensemble)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6b587e-de90-4b15-ac19-6713a515c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7137b6a-cde9-4502-971f-5b1a183db85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELM with cascaded InputToNode and default regressor\n",
    "#       _ _ _ _ _ _ _        _ _ _ _ _ _ _        _ _ _ _ _ _ _        \n",
    "#     |     (bip)    |     |    (base)    |     |               |       \n",
    "# ----|Input-to-Node1|-----|Input-to-Node2|-----|Node-to-Output |\n",
    "# u[n]| _ _ _ _ _ _ _|     | _ _ _ _ _ _ _|r'[n]| _ _ _ _ _ _ _ |\n",
    "#                                                       |\n",
    "#                                                       |\n",
    "#                                                  y[n] | y_pred\n",
    "# \n",
    "i2n = Pipeline([('bip', BatchIntrinsicPlasticity()), \n",
    "                ('base', InputToNode(bias_scaling=0.1))])\n",
    "casc_elm = ELMRegressor(input_to_node=i2n).fit(U, y)\n",
    "\n",
    "# Ensemble of InputToNode with activations\n",
    "#             _ _ _ _ _ _ _ \n",
    "#           |      (i)     |\n",
    "#      |----|Input-to-Node1|-----|\n",
    "#      |    | _ _ _ _ _ _ _|     |       _ _ _ _ _ _ _  \n",
    "#      |                          -----|               |\n",
    "# -----o                          r'[n]|Node-to-Output |------\n",
    "# u[n] |      _ _ _ _ _ _ _      |-----| _ _ _ _ _ _ _ |y[n]   \n",
    "#      |    |     (th)     |     |                      y_pred\n",
    "#      |----|Input-to-Node2|-----|\n",
    "#           | _ _ _ _ _ _ _|\n",
    "# \n",
    "i2n = FeatureUnion([('i', InputToNode(input_activation=\"identity\")), \n",
    "                    ('th', InputToNode(input_activation=\"tanh\"))])\n",
    "ens_elm = ELMRegressor(input_to_node=i2n)\n",
    "ens_elm.fit(U, y)\n",
    "print(casc_elm, ens_elm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e8c964-2ff0-4b74-a54a-d4f4a4279abd",
   "metadata": {},
   "source": [
    "## Echo State Networks\n",
    "\n",
    "ESNs, as variants of RNNs, consist of an \"Input-to-Node\", a \"Node-to-Node\" and a \"Node-to-Output\" block and are trained in three steps:\n",
    "\n",
    "1. Compute the neuron input states $\\mathbf{R}'$, which is the collection of reservoir states $\\mathbf{r}'[n]$. Note that here the activation function $f'(\\cdot)$ is typically linear.\n",
    "2. Compute the reservoir states $\\mathbf{R}$, which is the collection of reservoir states $\\mathbf{r}[n]$. Note that here the activation function $f(\\cdot)$ is typically non-linear.\n",
    "3. Compute the output weights $\\mathbf{W}^{\\mathrm{out}}$ using\n",
    "    1. Linear regression with $\\mathbf{R}$ when considering an ESN.\n",
    "    2. Backpropagation or other optimization algorithm when considering a CRN or when using an ESN with non-linear outputs.\n",
    "\n",
    "What follows is an example of how to construct such a vanilla ESN with PyRCN, where the ```ESNRegressor``` internally passes the input features through \"Input-to-Node\" and \"Node-to-Node\", and fits \"Node-to-Output\" using ```pyrcn.linear_model.IncrementalRegression```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3a617e-e695-4cb4-9e91-15f9b9a15599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrcn.echo_state_network import ESNRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572263e1-a883-44bc-b565-09d9360e0662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla ESN for regression tasks with spectral_radius and leakage\n",
    "#       _ _ _ _ _ _ _       _ _ _ _ _ _ _        _ _ _ _ _ _ _        \n",
    "#     |              |     |             |     |               |       \n",
    "# ----|Input-to-Node |-----|Node-to-Node |-----|Node-to-Output |\n",
    "# u[n]| _ _ _ _ _ _ _|r'[n]|_ _ _ _ _ _ _|r[n] | _ _ _ _ _ _ _ |\n",
    "#                                                      |\n",
    "#                                                      |\n",
    "#                                                 y[n] | y_pred\n",
    "# \n",
    "vanilla_esn = ESNRegressor(spectral_radius=1, leakage=0.9)\n",
    "vanilla_esn.fit(U, y)\n",
    "print(vanilla_esn.predict(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2363bf-21f6-48f0-9993-85e75d262c16",
   "metadata": {},
   "source": [
    "As for ELMs, various unsupervised learning techniques can be used to pre-train \"Input-to-Node\" and \"Node-to-Node\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b941d389-5e8f-420b-ad1d-b130d6ff4ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrcn.base.blocks import HebbianNodeToNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd45de00-ddc2-4408-ab2e-0fda43ec28ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom ESN with BatchIntrinsicPlasticity and HebbianNodeToNode\n",
    "#       _ _ _ _ _ _ _       _ _ _ _ _ _ _        _ _ _ _ _ _ _        \n",
    "#     |     (bip)    |     |   (hebb)    |     |               |       \n",
    "# ----|Input-to-Node |-----|Node-to-Node |-----|Node-to-Output |\n",
    "# u[n]| _ _ _ _ _ _ _|r'[n]|_ _ _ _ _ _ _|r[n] | _ _ _ _ _ _ _ |\n",
    "#                                                      |\n",
    "#                                                      |\n",
    "#                                                 y[n] | y_pred\n",
    "# \n",
    "bip_esn = ESNRegressor(input_to_node=BatchIntrinsicPlasticity(),\n",
    "                       node_to_node=HebbianNodeToNode(),\n",
    "                       regressor=Ridge(alpha=1e-5))\n",
    "\n",
    "bip_esn.fit(U, y)\n",
    "print(bip_esn.predict(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b60c37-6f12-4b59-9b0e-ad9aea6bf614",
   "metadata": {},
   "source": [
    "The term \"Deep ESN\" can refer to different approaches of hierarchical ESN architectures:\n",
    "\n",
    "Example of how to construct a rather complex ESN consisting of two layers. It is built out of two small parallel reservoirs in the first layer and a large reservoir in the second layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c7acd7-777e-499d-a479-4c38c1e1bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer ESN\n",
    "#                  u[n]\n",
    "#                   |\n",
    "#                   |\n",
    "#          _________o_________\n",
    "#         |                   |\n",
    "#   _ _ _ | _ _ _       _ _ _ | _ _ _ \n",
    "# |      (i)     |    |      (i)     |\n",
    "# |Input-to-Node1|    |Input-to-Node2|\n",
    "# | _ _ _ _ _ _ _|    | _ _ _ _ _ _ _|\n",
    "#         |r1'[n]             | r2'[n]\n",
    "#   _ _ _ | _ _ _       _ _ _ | _ _ _\n",
    "# |     (th)     |    |     (th)     |\n",
    "# | Node-to-Node1|    | Node-to-Node2|\n",
    "# | _ _ _ _ _ _ _|    | _ _ _ _ _ _ _|\n",
    "#         |r1[n]              | r2[n]\n",
    "#         |_____         _____|\n",
    "#               |       |\n",
    "#             _ | _ _ _ | _  \n",
    "#           |               |\n",
    "#           | Node-to-Node3 |\n",
    "#           | _ _ _ _ _ _ _ |\n",
    "#                   |\n",
    "#              r3[n]|\n",
    "#             _ _ _ | _ _ _  \n",
    "#           |               |\n",
    "#           |Node-to-Output |\n",
    "#           | _ _ _ _ _ _ _ |\n",
    "#                   |\n",
    "#               y[n]|\n",
    "\n",
    "l1 = Pipeline([('i2n1', InputToNode(hidden_layer_size=100)),\n",
    "               ('n2n1', NodeToNode(hidden_layer_size=100))])\n",
    "\n",
    "l2 = Pipeline([('i2n2', InputToNode(hidden_layer_size=400)),\n",
    "               ('n2n2', NodeToNode(hidden_layer_size=400))])\n",
    "\n",
    "i2n = FeatureUnion([('l1', l1),\n",
    "                    ('l2', l2)])\n",
    "n2n = NodeToNode(hidden_layer_size=500)\n",
    "layered_esn = ESNRegressor(input_to_node=i2n,\n",
    "                           node_to_node=n2n)\n",
    "\n",
    "layered_esn.fit(U, y)\n",
    "print(layered_esn.predict(U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e42b6-3044-40db-97c3-29f35c54a361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from pyrcn.base.blocks import PredefinedWeightsNodeToNode\n",
    "\n",
    "\n",
    "# Multiple small reservoirs with different leakages in parallel\n",
    "res1 = FeatureUnion([\n",
    "    (\"lambda_0.1\",\n",
    "     Pipeline([('i2n', InputToNode(hidden_layer_size=10)),\n",
    "               ('n2n', NodeToNode(hidden_layer_size=10,\n",
    "                                  leakage=0.1))])),\n",
    "    (\"lambda_0.2\",\n",
    "     Pipeline([('i2n', InputToNode(hidden_layer_size=10)),\n",
    "               ('n2n', NodeToNode(hidden_layer_size=10,\n",
    "                                  leakage=0.2))])),\n",
    "    (\"lambda_0.3\",\n",
    "     Pipeline([('i2n', InputToNode(hidden_layer_size=10)),\n",
    "               ('n2n', NodeToNode(hidden_layer_size=10,\n",
    "                                  leakage=0.3))])),\n",
    "    (\"lambda_0.4\",\n",
    "     Pipeline([('i2n', InputToNode(hidden_layer_size=10)),\n",
    "               ('n2n', NodeToNode(hidden_layer_size=10,\n",
    "                                  leakage=0.4))])),])\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "res2 = Pipeline([(\"i2n\", InputToNode(hidden_layer_size=100)),\n",
    "                 (\"n2n\", NodeToNode(hidden_layer_size=100))])\n",
    "\n",
    "i2n = FeatureUnion([(\"path1\",\n",
    "                     Pipeline([(\"res1\", res1), (\"pca\", pca),\n",
    "                               (\"res2\", res2)])),\n",
    "                    (\"path2\", res1)])\n",
    "\n",
    "n2n = PredefinedWeightsNodeToNode(\n",
    "    predefined_recurrent_weights=np.eye(40+100),\n",
    "    spectral_radius=0, leakage=1)\n",
    "\n",
    "deep_esn = ESNRegressor(input_to_node=i2n, node_to_node=n2n)\n",
    "deep_esn.fit(U, y)\n",
    "print(deep_esn.predict(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b938d8-1e1f-4b32-96f4-7108a6572bbc",
   "metadata": {},
   "source": [
    "## Complex example: Optimize the hyper-parameters of RCNs\n",
    "\n",
    "Example for a sequential parameter optimization with PyRCN. Therefore, a model with initial parameters and various search steps are defined. Internally, ```SequentialSearchCV``` will perform the list of optimization steps sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2687fc1-b465-464b-aa38-98e0da8afc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import (RandomizedSearchCV,\n",
    "                                     GridSearchCV)\n",
    "from scipy.stats import uniform\n",
    "from pyrcn.model_selection import SequentialSearchCV\n",
    "from pyrcn.datasets import mackey_glass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5353ba84-304f-4848-b332-1d3ba1d0e1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "X, y = mackey_glass(n_timesteps=5000)\n",
    "X_train, X_test = X[:1900], X[1900:]\n",
    "y_train, y_test = y[:1900], y[1900:]\n",
    "\n",
    "# Define initial ESN model\n",
    "esn = ESNRegressor(bias_scaling=0, spectral_radius=0, leakage=1)\n",
    "\n",
    "# Define optimization workflow\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "step_1_params = {'input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                 'spectral_radius': uniform(loc=0, scale=2)}\n",
    "kwargs_1 = {'n_iter': 200, 'n_jobs': -1, 'scoring': scorer, \n",
    "            'cv': TimeSeriesSplit()}\n",
    "step_2_params = {'leakage': [0.2, 0.4, 0.7, 0.9, 1.0]}\n",
    "kwargs_2 = {'verbose': 5, 'scoring': scorer, 'n_jobs': -1,\n",
    "            'cv': TimeSeriesSplit()}\n",
    "\n",
    "searches = [('step1', RandomizedSearchCV, step_1_params, kwargs_1),\n",
    "            ('step2', GridSearchCV, step_2_params, kwargs_2)]\n",
    "\n",
    "# Perform the search\n",
    "esn_opti = SequentialSearchCV(esn, searches).fit(X_train.reshape(-1, 1), y_train)\n",
    "print(esn_opti)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d477ddf-8ed3-4733-83c0-478ac0dda312",
   "metadata": {},
   "source": [
    "## Programming pattern for sequence processing\n",
    "\n",
    "This complex use-case requires a serious hyper-parameter tuning. To keep the code example simple, we did not include the optimization in this paper and refer the interested readers to the Jupyter Notebook [^1] that was developed to produce these results.\n",
    "\n",
    "[^1]: https://github.com/TUD-STKS/PyRCN/blob/main/examples/digits.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d53e21-e909-4954-8dfa-4e119ccf7a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from pyrcn.echo_state_network import ESNClassifier\n",
    "from pyrcn.metrics import accuracy_score\n",
    "from pyrcn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b784cca-389e-4f24-b1cc-eb46b7280c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "X, y = load_digits(return_X_y=True, as_sequence=True)\n",
    "print(\"Number of digits: {0}\".format(len(X)))\n",
    "print(\"Shape of digits {0}\".format(X[0].shape))\n",
    "# Divide the dataset into training and test subsets\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, \n",
    "                                          random_state=42)\n",
    "print(\"Number of digits in training set: {0}\".format(len(X_tr)))\n",
    "print(\"Shape of the first digit: {0}\".format(X_tr[0].shape))\n",
    "print(\"Number of digits in test set: {0}\".format(len(X_te)))\n",
    "print(\"Shape of the first digit: {0}\".format(X_te[0].shape))\n",
    "\n",
    "# These parameters were optimized using SequentialSearchCV\n",
    "esn_params = {'input_scaling': 0.05077514155476392,\n",
    "              'spectral_radius': 1.1817858863764836,\n",
    "              'input_activation': 'identity',\n",
    "              'k_in': 5,\n",
    "              'bias_scaling': 1.6045393364745582,\n",
    "              'reservoir_activation': 'tanh',\n",
    "              'leakage': 0.03470266988650412,\n",
    "              'k_rec': 10,\n",
    "              'alpha': 3.0786517836196185e-05,\n",
    "              'decision_strategy': \"winner_takes_all\"}\n",
    "\n",
    "b_esn = ESNClassifier(**esn_params)\n",
    "\n",
    "param_grid = {'hidden_layer_size': [50, 100, 200, 400, 500],\n",
    "              'bidirectional': [False, True]}\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    esn_cv = cross_validate(clone(b_esn).set_params(**params), \n",
    "                            X=X_tr, y=y_tr,\n",
    "                            scoring=make_scorer(accuracy_score))\n",
    "    esn = clone(b_esn).set_params(**params).fit(X_tr, y_tr, n_jobs=1)\n",
    "    acc_score = accuracy_score(y_te, esn.predict(X_te))\n",
    "    print(acc_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
