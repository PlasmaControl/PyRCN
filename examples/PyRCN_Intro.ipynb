{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4a077e7-ac2b-451a-9e3e-4645f408ef41",
   "metadata": {},
   "source": [
    "# Building blocks of Reservoir Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c02949-1b19-49fb-aac6-27063f5f87be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrcn.base.blocks import InputToNode\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c9bd2-7ea4-49f7-b8a3-c93ff62ff52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a toy dataset\n",
    "U, y = make_blobs(n_samples=100, n_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8a67dc-f089-44d5-8d5f-3ac67158e7d0",
   "metadata": {},
   "source": [
    "## Input-to-Node\n",
    "\n",
    "The \"Input-to-Node\" component describes the connections from the input features to the reservoir and the activation functions of the reservoir neurons. Normally, the input weight matrix $\\mathbf{W}^{\\mathrm{in}}$ has the dimension of $N^{\\mathrm{res}}\\times N^{\\mathrm{in}}$, where $N^{\\mathrm{res}}$ and $N^{\\mathrm{in}}$ are the size of the reservoir and dimension of the input feature vector $\\mathbf{u}[n]$ with the time index $n$, respectively. With \n",
    "\n",
    "\\begin{align}\n",
    "    \\label{eq:InputToNode}\n",
    "    \\mathbf{r}'[n] = f'(\\mathbf{W}^{\\mathrm{in}}\\mathbf{u}[n] + \\mathbf{w}^{\\mathrm{bi}}) \\text{ , }\n",
    "\\end{align}\n",
    "\n",
    "we can describe the non-linear projection of the input features $\\mathbf{u}[n]$ into the high-dimensional reservoir space $\\mathbf{r}'[n]$ via the non-linear input activation function $f'(\\cdot)$. \n",
    "\n",
    "The values inside the input weight matrix are usually initialized randomly from a uniform distribution on the interval $[-1, 1]$ and are afterwards scaled using the input scaling factor $\\alpha_{\\mathrm{u}}$. Since in case of a high dimensional input feature space and/or large reservoir sizes $N^{\\mathrm{res}}$, this leads to a huge input weight matrix and expensive computations to feed the feature vectors into the reservoir, it was shown that it is sufficient to have only a very small number of connections from the input nodes to the nodes inside the reservoir. Each node of the reservoir may therefore be connected to only $K^{\\mathrm{in}}$ ($\\ll N^{\\mathrm{in}}$) randomly selected input entries. This makes $\\mathbf{W}^{\\mathrm{in}}$ typically very sparse and feeding the feature vectors into the reservoir potentially more efficient. \n",
    "\n",
    "The bias weights $\\mathbf{w}^{\\mathrm{bi}}$ with dimension $N^{\\mathrm{res}}$ are typically initialized by fixed random values from a uniform distribution between $\\pm 1$ and multiplied by the hyper-parameter $\\alpha_{\\mathrm{bi}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75174c87-d3d5-4dfe-b76e-549b1f86028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#      _ _ _ _ _ _ _ _\n",
    "#     |               |\n",
    "# ----| Input-to-Node |------\n",
    "# u[n]|_ _ _ _ _ _ _ _|r'[n]\n",
    "# U                    R_i2n\n",
    "\n",
    "input_to_node = InputToNode(hidden_layer_size=50,\n",
    "                            k_in=5, input_activation=\"tanh\",\n",
    "                            input_scaling=1.0, bias_scaling=0.1)\n",
    "\n",
    "R_i2n = input_to_node.fit_transform(U)\n",
    "print(U.shape, R_i2n.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448868fb-e718-434a-b350-be85c0a1c328",
   "metadata": {},
   "source": [
    "## Node-to-Node\n",
    "\n",
    "The \"Node-to-Node\" component describes the connections inside the reservoir. The output of \"Input-to-Node\" $\\mathbf{r}'[n]$ together with the output of \"Node-to-Node\" from the previous time step $\\mathbf{r}[n-1]$ are used to compute the new output of \"Node-to-Node\" $\\mathbf{r}[n]$ using \n",
    "\n",
    "\\begin{align}\n",
    "    \\label{eq:NodeToNode}\n",
    "    \\mathbf{r}[n] = (1-\\lambda)\\mathbf{r}[n-1] + \\lambda f(\\mathbf{r}'[n] + \\mathbf{W}^{\\mathrm{res}}\\mathbf{r}[n-1])\\text{ , } \n",
    "\\end{align}\n",
    "\n",
    "which is a leaky integration of the time-dependent reservoir states $\\mathbf{r}[n]$. $f(\\cdot)$ acts as the non-linear reservoir activation functions of the neurons in \"Node-to-Node\". The leaky integration is equivalent to a first-order lowpass filter. Depending  on the leakage $\\lambda \\in (0, 1]$, the reservoir states are globally smoothed.\n",
    "\n",
    "The reservoir weight matrix $\\mathbf{W}^{\\mathrm{res}}$ is a square matrix of the size $N^{\\mathrm{res}}$. These weights are typically initialized from a standard normal distribution. The Echo State Property (ESP) requires that the states of all reservoir neurons need to decay in a finite time for a finite input pattern. In order to fulfill the ESP, the reservoir weight matrix is typically normalized by its largest absolute eigenvalue and rescaled to a spectral radius $\\rho$, because it was shown that the ESP holds as long as $\\rho \\le 1$. The spectral radius and the leakage together shape the temporal memory of the reservoir. Similar as for \"Input-to-Node\", the reservoir weight matrix gets huge in case of large reservoir sizes $N^{\\mathrm{res}}$, it can be sufficient to only connect each node in the reservoir only to $K^{\\mathrm{rec}}$ ($\\ll N^{\\mathrm{res}}$) randomly selected other nodes in the reservoir, and to set the remaining weights to zero.\n",
    "\n",
    "To incorporate some information from the future inputs, bidirectional RCNs have been introduced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40413386-483b-47a3-8e10-ad1102d45122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrcn.base.blocks import NodeToNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505740ad-aa48-4631-a8c0-4b1c9b1474ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#      _ _ _ _ _ _ _ _        _ _ _ _ _ _ _\n",
    "#     |               |      |              |\n",
    "# ----| Input-to-Node |------| Node-to-Node |------\n",
    "# u[n]|_ _ _ _ _ _ _ _|r'[n] |_ _ _ _ _ _ _ |r[n]\n",
    "# U                    R_i2n                 R_n2n\n",
    "\n",
    "# Initialize, fit and apply NodeToNode\n",
    "node_to_node = NodeToNode(hidden_layer_size=50,\n",
    "                          reservoir_activation=\"tanh\",\n",
    "                          spectral_radius=1.0, leakage=0.9,\n",
    "                          bidirectional=False)\n",
    "R_n2n = node_to_node.fit_transform(R_i2n)\n",
    "print(U.shape, R_n2n.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe76638-0482-4f8c-a261-05e376f7da86",
   "metadata": {},
   "source": [
    "## Node-to-Output\n",
    "\n",
    "The \"Node-to-Output\" component is the mapping of the reservoir state $\\mathbf{r}[n]$ to the output $\\mathbf{y}[n]$ of the network. In conventional RCNs, this mapping is trained using (regularized) linear regression. To that end, all reservoir states $\\mathbf{r}[n]$ are concatenated into the reservoir state collection matrix $\\mathbf{R}$. As linear regression usually contains an intercept term, every reservoir state $\\mathbf{r}[n]$ is expanded by a constant of 1. All desired outputs $\\mathbf{d}[n]$ are collected into the desired output collection matrix $\\mathbf{D}$. Then, the mapping matrix $\\mathbf{W}^{\\mathrm{out}}$ can be computed using\n",
    "\n",
    "\\begin{align}\n",
    "    \\label{eq:linearRegression}\n",
    "    \\mathbf{W}^{\\mathrm{out}} =\\left(\\mathbf{R}\\mathbf{R}^{\\mathrm{T}} + \\epsilon\\mathbf{I}\\right)^{-1}(\\mathbf{D}\\mathbf{R}^{\\mathrm{T}}) \\text{,}\n",
    "\\end{align}\n",
    "\n",
    "where $\\epsilon$ is a regularization parameter.\n",
    "\n",
    "The size of the output weight matrix $N^{\\mathrm{out}}\\times (N^{\\mathrm{res}} + 1)$ or $N^{\\mathrm{out}}\\times (2 \\times N^{\\mathrm{res}} + 1)$ in case of a bidirectional \"Node-to-Node\" determines the total number of free parameters to be trained in the neural network. \n",
    "\n",
    "After training, the output $\\mathbf{y}[n]$ can be computed using Equation \n",
    "\n",
    "\\begin{align}\n",
    "\\label{eq:readout}\n",
    "\\mathbf{y}[n] = \\mathbf{W}^{\\mathrm{out}}\\mathbf{r}[n] \\text{ . }\n",
    "\\end{align}\n",
    "\n",
    "Note that, in general, other training methodologies could be used to compute output weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d893a43-bfbd-48e4-bf79-c20de509fcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59d9f68-5eca-4514-943e-2a8206738ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#       _ _ _ _ _ _ _       _ _ _ _ _ _ _        _ _ _ _ _ _ _        \n",
    "#     |              |     |             |     |               |       \n",
    "# ----|Input-to-Node |-----|Node-to-Node |-----|Node-to-Output |\n",
    "# u[n]| _ _ _ _ _ _ _|r'[n]|_ _ _ _ _ _ _|r[n] | _ _ _ _ _ _ _ |\n",
    "# U                   R_i2n               R_n2n        |\n",
    "#                                                      |\n",
    "#                                                 y[n] | y_pred\n",
    "\n",
    "# Initialize, fit and apply NodeToOutput\n",
    "y_pred = Ridge().fit(R_n2n, y).predict(R_n2n)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fd7e1f-d735-4272-8d45-6772a842104b",
   "metadata": {},
   "source": [
    "## Predicting the Mackey-Glass equation\n",
    "\n",
    "Set up and train vanilla RCNs for predicting the Mackey-Glass time series with the same settings as used to introduce ESNs. The minimum working example shows the simplicity of implementing a model with PyRCN and the inter-operability with scikit-learn; it needs only four lines of code to load the Mackey-Glass dataset that is part of PyRCN and only two lines to fit the different RCN models, respectively. Instead of the default incremental regression, we have customized the ```ELMRegressor()``` by using ```Ridge``` from scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9495fcd-cfa1-4a25-8cf7-11d26ee1b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge as skRidge\n",
    "from pyrcn.echo_state_network import ESNRegressor\n",
    "from pyrcn.extreme_learning_machine import ELMRegressor\n",
    "from pyrcn.datasets import mackey_glass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a73dea-b14e-4532-bac1-532d15158436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "X, y = mackey_glass(n_timesteps=5000)\n",
    "# Define Train/Test lengths\n",
    "trainLen = 1900\n",
    "X_train, y_train = X[:trainLen], y[:trainLen]\n",
    "X_test, y_test = X[trainLen:], y[trainLen:]\n",
    "\n",
    "# Initialize and train an ELMRegressor and an ESNRegressor\n",
    "esn = ESNRegressor().fit(X=X_train.reshape(-1, 1), y=y_train)\n",
    "elm = ELMRegressor(regressor=skRidge()).fit(X=X_train.reshape(-1, 1), y=y_train)\n",
    "\n",
    "print(\"Fitted models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694acc81-63d0-40e2-8dbc-051f930ee90d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build Reservoir Computing Networks with PyRCN\n",
    "\n",
    "By combining the building blocks introduced above, a vast number of different RCNs can be constructed. In this section, we build two important variants of RCNs, namely ELMs and ESNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0478bfab-1e56-4cba-91c2-e788d7f52cd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extreme Learning Machines\n",
    "The vanilla ELM as a single-layer feedforward network consists of an \"Input-to-Node\" and a \"Node-to-Output\" module and is trained in two steps: \n",
    "\n",
    "1. Compute the high-dimensional reservoir states $\\mathbf{R}'$, which is the collection of reservoir states $\\mathbf{r}'[n]$.\n",
    "2. Compute the output weights $\\mathbf{W}^{\\mathrm{out}}$ with $\\mathbf{R}'$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1794e4-de8c-4332-900e-6b3b6cc6b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, y = make_blobs(n_samples=100, n_features=10)\n",
    "from pyrcn.extreme_learning_machine import ELMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145ea0b3-a736-41e5-a01d-12507804929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla ELM for regression tasks with input_scaling\n",
    "#       _ _ _ _ _ _ _        _ _ _ _ _ _ _        \n",
    "#     |              |     |               |       \n",
    "# ----|Input-to-Node |-----|Node-to-Output |------\n",
    "# u[n]| _ _ _ _ _ _ _|r'[n]| _ _ _ _ _ _ _ |y[n]\n",
    "#                                           y_pred\n",
    "# \n",
    "vanilla_elm = ELMRegressor(input_scaling=0.9)\n",
    "vanilla_elm.fit(U, y)\n",
    "print(vanilla_elm.predict(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa0d86-2b80-4097-a779-607567eabb2b",
   "metadata": {},
   "source": [
    "Example of how to construct an ELM with a BIP \"Input-to-Node\" ELMs with PyRCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8303d3-a375-46dc-b5a8-575f413903d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrcn.base.blocks import BatchIntrinsicPlasticity\n",
    "\n",
    "# Custom ELM with BatchIntrinsicPlasticity\n",
    "#       _ _ _ _ _ _ _        _ _ _ _ _ _ _        \n",
    "#     |              |     |               |       \n",
    "# ----|     BIP      |-----|Node-to-Output |------\n",
    "# u[n]| _ _ _ _ _ _ _|r'[n]| _ _ _ _ _ _ _ |y[n]\n",
    "#                                           y_pred\n",
    "# \n",
    "bip_elm = ELMRegressor(input_to_node=BatchIntrinsicPlasticity(),\n",
    "                       regressor=Ridge(alpha=1e-5))\n",
    "\n",
    "bip_elm.fit(U, y)\n",
    "print(bip_elm.predict(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd937e03-dc9e-4e16-966a-5b0888398273",
   "metadata": {},
   "source": [
    "Hierarchical or Ensemble ELMs can then be built using multiple \"Input-to-Node\" modules in parallel or in a cascade. This is possible when using using scikit-learn's ```sklearn.pipeline.Pipeline``` (cascading) or ```sklearn.pipeline.FeatureUnion``` (ensemble). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fb50c1-0625-4bdc-aec5-06f39bfca47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c006bc-590f-41ee-8066-16692e383001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELM with cascaded InputToNode and default regressor\n",
    "#       _ _ _ _ _ _ _        _ _ _ _ _ _ _        _ _ _ _ _ _ _        \n",
    "#     |     (bip)    |     |    (base)    |     |               |       \n",
    "# ----|Input-to-Node1|-----|Input-to-Node2|-----|Node-to-Output |\n",
    "# u[n]| _ _ _ _ _ _ _|     | _ _ _ _ _ _ _|r'[n]| _ _ _ _ _ _ _ |\n",
    "#                                                       |\n",
    "#                                                       |\n",
    "#                                                  y[n] | y_pred\n",
    "# \n",
    "i2n = Pipeline([('bip', BatchIntrinsicPlasticity()), \n",
    "                ('base', InputToNode(bias_scaling=0.1))])\n",
    "casc_elm = ELMRegressor(input_to_node=i2n).fit(U, y)\n",
    "\n",
    "# Ensemble of InputToNode with activations\n",
    "#             _ _ _ _ _ _ _ \n",
    "#           |      (i)     |\n",
    "#      |----|Input-to-Node1|-----|\n",
    "#      |    | _ _ _ _ _ _ _|     |       _ _ _ _ _ _ _  \n",
    "#      |                          -----|               |\n",
    "# -----o                          r'[n]|Node-to-Output |------\n",
    "# u[n] |      _ _ _ _ _ _ _      |-----| _ _ _ _ _ _ _ |y[n]   \n",
    "#      |    |     (th)     |     |                      y_pred\n",
    "#      |----|Input-to-Node2|-----|\n",
    "#           | _ _ _ _ _ _ _|\n",
    "# \n",
    "i2n = FeatureUnion([('i', InputToNode(input_activation=\"identity\")), \n",
    "                    ('th', InputToNode(input_activation=\"tanh\"))])\n",
    "ens_elm = ELMRegressor(input_to_node=i2n)\n",
    "ens_elm.fit(U, y)\n",
    "print(casc_elm, ens_elm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc04e75-ea69-4ae8-9815-622583b7e5dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Echo State Networks\n",
    "ESNs, as variants of RNNs, consist of an \"Input-to-Node\", a \"Node-to-Node\" and a \"Node-to-Output\" module and are trained in three steps. \n",
    "\n",
    "1. Compute the neuron input states $\\mathbf{R}'$, which is the collection of reservoir states $\\mathbf{r}'[n]$. Note that here the activation function $f'(\\cdot)$ is typically linear.\n",
    "2. Compute the reservoir states $\\mathbf{R}$, which is the collection of reservoir states $\\mathbf{r}[n]$. Note that here the activation function $f(\\cdot)$ is typically non-linear.\n",
    "3. Compute the output weights $\\mathbf{W}^{\\mathrm{out}}$ using\n",
    "    1. Linear regression with $\\mathbf{R}$ when considering an ESN.\n",
    "    2. Backpropagation or other optimization algorithm when considering a CRN or when using an ESN with non-linear outputs.\n",
    "    \n",
    "What follows is an example of how to construct such a vanilla ESN with PyRCN, where the ```ESNRegressor``` internally passes the input features through \"Input-to-Node\" and \"Node-to-Node\", and trains \"Node-to-Output\" using ```pyrcn.linear_model.IncrementalRegression```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17fcc4c-9e3e-4369-adab-450a48b9945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrcn.echo_state_network import ESNRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75794b22-fbb1-4bd8-8dca-1357c86c2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla ESN for regression tasks with spectral_radius and leakage\n",
    "#       _ _ _ _ _ _ _       _ _ _ _ _ _ _        _ _ _ _ _ _ _        \n",
    "#     |              |     |             |     |               |       \n",
    "# ----|Input-to-Node |-----|Node-to-Node |-----|Node-to-Output |\n",
    "# u[n]| _ _ _ _ _ _ _|r'[n]|_ _ _ _ _ _ _|r[n] | _ _ _ _ _ _ _ |\n",
    "#                                                      |\n",
    "#                                                      |\n",
    "#                                                 y[n] | y_pred\n",
    "# \n",
    "vanilla_esn = ESNRegressor(spectral_radius=1, leakage=0.9)\n",
    "vanilla_esn.fit(U, y)\n",
    "print(vanilla_esn.predict(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1a2354-7492-4742-a68d-67db261a37e8",
   "metadata": {},
   "source": [
    "As for ELMs, various unsupervised learning techniques can be used to pre-train \"Input-to-Node\" and \"Node-to-Node\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3952a4-5f0b-4fc8-b901-c8ba18d6b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrcn.base.blocks import HebbianNodeToNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a8ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom ESN with BatchIntrinsicPlasticity and HebbianNodeToNode\n",
    "#       _ _ _ _ _ _ _       _ _ _ _ _ _ _        _ _ _ _ _ _ _        \n",
    "#     |     (bip)    |     |   (hebb)    |     |               |       \n",
    "# ----|Input-to-Node |-----|Node-to-Node |-----|Node-to-Output |\n",
    "# u[n]| _ _ _ _ _ _ _|r'[n]|_ _ _ _ _ _ _|r[n] | _ _ _ _ _ _ _ |\n",
    "#                                                      |\n",
    "#                                                      |\n",
    "#                                                 y[n] | y_pred\n",
    "# \n",
    "bip_esn = ESNRegressor(input_to_node=BatchIntrinsicPlasticity(),\n",
    "                       node_to_node=HebbianNodeToNode(),\n",
    "                       regressor=Ridge(alpha=1e-5))\n",
    "\n",
    "bip_esn.fit(U, y)\n",
    "print(bip_esn.predict(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d4523f-1216-4d86-8dec-3ea0dcee335d",
   "metadata": {},
   "source": [
    "The term \"Deep ESN\" can refer to different approaches of hierarchical ESN architectures: \n",
    "\n",
    "Example of how to construct a rather complex ESN consisting of two layers. It is built out of two small parallel reservoirs in the first layer and a large reservoir in the second layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd3e38f-2c87-4403-a7f6-863357bfab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer ESN\n",
    "#                  u[n]\n",
    "#                   |\n",
    "#                   |\n",
    "#          _________o_________\n",
    "#         |                   |\n",
    "#   _ _ _ | _ _ _       _ _ _ | _ _ _ \n",
    "# |      (i)     |    |      (i)     |\n",
    "# |Input-to-Node1|    |Input-to-Node2|\n",
    "# | _ _ _ _ _ _ _|    | _ _ _ _ _ _ _|\n",
    "#         |r1'[n]             | r2'[n]\n",
    "#   _ _ _ | _ _ _       _ _ _ | _ _ _\n",
    "# |     (th)     |    |     (th)     |\n",
    "# | Node-to-Node1|    | Node-to-Node2|\n",
    "# | _ _ _ _ _ _ _|    | _ _ _ _ _ _ _|\n",
    "#         |r1[n]              | r2[n]\n",
    "#         |_____         _____|\n",
    "#               |       |\n",
    "#             _ | _ _ _ | _  \n",
    "#           |               |\n",
    "#           | Node-to-Node3 |\n",
    "#           | _ _ _ _ _ _ _ |\n",
    "#                   |\n",
    "#              r3[n]|\n",
    "#             _ _ _ | _ _ _  \n",
    "#           |               |\n",
    "#           |Node-to-Output |\n",
    "#           | _ _ _ _ _ _ _ |\n",
    "#                   |\n",
    "#               y[n]|\n",
    "\n",
    "l1 = Pipeline([('i2n1', InputToNode(hidden_layer_size=100)),\n",
    "               ('n2n1', NodeToNode(hidden_layer_size=100))])\n",
    "\n",
    "l2 = Pipeline([('i2n2', InputToNode(hidden_layer_size=400)),\n",
    "               ('n2n2', NodeToNode(hidden_layer_size=400))])\n",
    "\n",
    "i2n = FeatureUnion([('l1', l1),\n",
    "                    ('l2', l2)])\n",
    "n2n = NodeToNode(hidden_layer_size=500)\n",
    "layered_esn = ESNRegressor(input_to_node=i2n,\n",
    "                           node_to_node=n2n)\n",
    "\n",
    "layered_esn.fit(U, y)\n",
    "print(layered_esn.predict(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314f436a-cf1a-45e1-b193-e1defe98c5e7",
   "metadata": {},
   "source": [
    "## Complex example: Optimize the hyper-parameters of RCNs\n",
    "\n",
    "Example for a sequential parameter optimization with PyRCN. Therefore, a model with initial parameters and various search steps are defined. Internally, ```SequentialSearchCV``` will perform the list of optimization steps sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03e31ee-d74b-437a-adf5-632aa3b7fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import RandomizedSearchCV, \\\n",
    "                                    GridSearchCV\n",
    "from scipy.stats import uniform\n",
    "from pyrcn.model_selection import SequentialSearchCV\n",
    "from pyrcn.datasets import mackey_glass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c3f40e-beb4-44ab-b65d-3ffdab6d9ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "X, y = mackey_glass(n_timesteps=5000)\n",
    "X_train, X_test = X[:1900], X[1900:]\n",
    "y_train, y_test = y[:1900], y[1900:]\n",
    "\n",
    "# Define initial ESN model\n",
    "esn = ESNRegressor(bias_scaling=0, spectral_radius=0, leakage=1)\n",
    "\n",
    "# Define optimization workflow\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "step_1_params = {'input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                 'spectral_radius': uniform(loc=0, scale=2)}\n",
    "kwargs_1 = {'n_iter': 200, 'n_jobs': -1, 'scoring': scorer, \n",
    "            'cv': TimeSeriesSplit()}\n",
    "step_2_params = {'leakage': [0.2, 0.4, 0.7, 0.9, 1.0]}\n",
    "kwargs_2 = {'verbose': 5, 'scoring': scorer, 'n_jobs': -1,\n",
    "            'cv': TimeSeriesSplit()}\n",
    "\n",
    "searches = [('step1', RandomizedSearchCV, step_1_params, kwargs_1),\n",
    "            ('step2', GridSearchCV, step_2_params, kwargs_2)]\n",
    "\n",
    "# Perform the search\n",
    "esn_opti = SequentialSearchCV(esn, searches).fit(X_train.reshape(-1, 1), y_train)\n",
    "print(esn_opti)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576ec7c4-836f-45e0-a0f6-7ed37493803e",
   "metadata": {},
   "source": [
    "## Programming pattern for sequence processing\n",
    "\n",
    "This complex use-case requires a serious hyper-parameter tuning. To keep the code example simple, we did not include the optimization in this paper and refer the interested readers to the Jupyter Notebook [^1] that was developed to produce these results.\n",
    "\n",
    "[^1]: https://github.com/TUD-STKS/PyRCN/blob/master/examples/digits.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7191dc16-dd0a-4433-be4b-788a46c0ed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from pyrcn.echo_state_network import ESNClassifier\n",
    "from pyrcn.metrics import accuracy_score\n",
    "from pyrcn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf17b70-68a7-4116-9b23-0c586ce90775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "X, y = load_digits(return_X_y=True, as_sequence=True)\n",
    "print(\"Number of digits: {0}\".format(len(X)))\n",
    "print(\"Shape of digits {0}\".format(X[0].shape))\n",
    "# Divide the dataset into training and test subsets\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, \n",
    "                                          random_state=42)\n",
    "print(\"Number of digits in training set: {0}\".format(len(X_train)))\n",
    "print(\"Shape of the first digit: {0}\".format(X_train[0].shape))\n",
    "print(\"Number of digits in test set: {0}\".format(len(X_test)))\n",
    "print(\"Shape of the first digit: {0}\".format(X_test[0].shape))\n",
    "\n",
    "# These parameters were optimized using SequentialSearchCV\n",
    "esn_params = {'input_scaling': 0.1,\n",
    "              'spectral_radius': 1.2,\n",
    "              'input_activation': 'identity',\n",
    "              'k_in': 5,\n",
    "              'bias_scaling': 0.5,\n",
    "              'reservoir_activation': 'tanh',\n",
    "              'leakage': 0.1,\n",
    "              'k_rec': 10,\n",
    "              'alpha': 1e-5,\n",
    "              'decision_strategy': \"winner_takes_all\"}\n",
    "\n",
    "b_esn = ESNClassifier(**esn_params)\n",
    "\n",
    "param_grid = {'hidden_layer_size': [50, 100, 200, 400, 500],\n",
    "              'bidirectional': [False, True]}\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    esn_cv = cross_validate(clone(b_esn).set_params(**params), \n",
    "                            X=X_tr, y=y_tr,\n",
    "                            scoring=make_scorer(accuracy_score))\n",
    "    esn = clone(b_esn).set_params(**params).fit(X_tr, y_tr, n_jobs=-1)\n",
    "    acc_score = accuracy_score(y_te, esn.predict(X_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fca637-9b2e-4b88-9316-e14a7d6059e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
