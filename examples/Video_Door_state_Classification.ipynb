{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "cwd = os.getcwd()\n",
    "module_path = os.path.dirname(cwd)  # target working directory\n",
    "\n",
    "sys.path = [item for item in sys.path if item != module_path]  # remove module_path from sys.path\n",
    "sys.path.append(module_path)  # add module_path to sys.path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from pyrcn.base import InputToNode, NodeToNode\n",
    "from pyrcn.linear_model import IncrementalRegression\n",
    "from pyrcn.echo_state_network import SeqToSeqESNClassifier\n",
    "from pyrcn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from pyrcn.model_selection import SequentialSearchCV\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from scipy.stats import uniform\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(fname,Nfr=-1):\n",
    "    tmp= open(fname+'.txt', 'rb');a=tmp.read();tmp.close();T=len(a) # Just to know how many frames (T) are there in the file\n",
    "    if Nfr!=-1:\n",
    "        T=np.min((T,Nfr))\n",
    "    dim=[30,30] # Dimension of each frame\n",
    "    N_fr=dim[0]*dim[1] # size of the input vector\n",
    "    yuvfile= open(fname+'.yuv', 'rb') # Opening the video file\n",
    "    door_state_file= open(fname+'.txt', 'rb') # Opening the annotation file\n",
    "    TARGET=np.zeros((T, ))\n",
    "    FRAMES=np.zeros((T,N_fr))\n",
    "    for t in tqdm(range(T)): # for each frame    \n",
    "        fr2=np.zeros(N_fr) \n",
    "        frame = yuvfile.read(N_fr)\n",
    "        for i in range(N_fr):\n",
    "            fr2[i]=frame[i]\n",
    "        # ----------------------------------    \n",
    "        fr2=fr2/255.0 # Normalizing the pixel values to [0,1]\n",
    "        FRAMES[t,:]=fr2\n",
    "        TARGET[t] = int(door_state_file.read(1))\n",
    "    return FRAMES, TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 251654/251654 [00:28<00:00, 8957.08it/s]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    X_train, X_test, y_train, y_test = load(r\"E:\\RCN_CICSyN2015\\Seq_video_dataset.joblib\")\n",
    "except FileNotFoundError:\n",
    "    n_files = 1\n",
    "\n",
    "    X_total = [None] * n_files\n",
    "    y_total = [None] * n_files\n",
    "    n_sequences_total = [None] * n_files\n",
    "    for k in range(n_files):\n",
    "        X_total[k], y_total[k] = read_file(r\"E:\\RCN_CICSyN2015\\Seq_\" + str(k + 1))\n",
    "\n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "    X_test_list = []\n",
    "    y_test_list = []\n",
    "\n",
    "    for k in range(n_files):\n",
    "        n_sequences_total[k] = int(len(X_total[k]) / 1800)\n",
    "        X_total[k] = np.array_split(X_total[k], n_sequences_total[k])\n",
    "        y_total[k] = np.array_split(y_total[k], n_sequences_total[k])\n",
    "        for m, (X, y) in enumerate(zip(X_total[k], y_total[k])):\n",
    "            if m < int(.5*n_sequences_total[k]):\n",
    "                X_train_list.append(X)\n",
    "                y_train_list.append(y)\n",
    "            else:\n",
    "                X_test_list.append(X)\n",
    "                y_test_list.append(y)\n",
    "\n",
    "    X_train = np.empty(shape=(len(X_train_list), ), dtype=object)\n",
    "    y_train = np.empty(shape=(len(y_train_list), ), dtype=object)\n",
    "    X_test = np.empty(shape=(len(X_test_list), ), dtype=object)\n",
    "    y_test = np.empty(shape=(len(y_test_list), ), dtype=object)\n",
    "\n",
    "    for k, (X, y) in enumerate(zip(X_train_list, y_train_list)):\n",
    "        X_train[k] = X.astype(float)\n",
    "        y_train[k] = y.astype(float)\n",
    "\n",
    "    for k, (X, y) in enumerate(zip(X_test_list, y_test_list)):\n",
    "        X_test[k] = X.astype(float)\n",
    "        y_test[k] = y.astype(float)\n",
    "    \n",
    "    dump([X_train, X_test, y_train, y_test], r\"E:\\RCN_CICSyN2015\\Seq_video_dataset.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((69,), (1811, 900), (69,), (1811,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_train[0].shape, y_train.shape, y_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70,), (1810, 900), (70,), (1810,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, X_test[0].shape, y_test.shape, y_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 16.8min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 39.6min\n"
     ]
    }
   ],
   "source": [
    "initially_fixed_params = {'hidden_layer_size': 50,\n",
    "                          'k_in': 10,\n",
    "                          'input_scaling': 0.4,\n",
    "                          'input_activation': 'identity',\n",
    "                          'bias_scaling': 0.0,\n",
    "                          'spectral_radius': 0.0,\n",
    "                          'leakage': 0.1,\n",
    "                          'k_rec': 10,\n",
    "                          'reservoir_activation': 'tanh',\n",
    "                          'bi_directional': False,\n",
    "                          'wash_out': 0,\n",
    "                          'continuation': False,\n",
    "                          'alpha': 1e-3,\n",
    "                          'random_state': 42}\n",
    "\n",
    "step1_esn_params = {'input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'spectral_radius': uniform(loc=0, scale=2)}\n",
    "\n",
    "step2_esn_params = {'leakage': loguniform(1e-5, 1e0)}\n",
    "step3_esn_params = {'bias_scaling': np.linspace(0.0, 1.0, 11)}\n",
    "step4_esn_params = {'alpha': loguniform(1e-5, 1e1)}\n",
    "\n",
    "kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "kwargs_step3 = {'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "kwargs_step4 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "\n",
    "# The searches are defined similarly to the steps of a sklearn.pipeline.Pipeline:\n",
    "searches = [('step1', RandomizedSearchCV, step1_esn_params, kwargs_step1),\n",
    "            ('step2', RandomizedSearchCV, step2_esn_params, kwargs_step2),\n",
    "            ('step3', GridSearchCV, step3_esn_params, kwargs_step3),\n",
    "            ('step4', RandomizedSearchCV, step4_esn_params, kwargs_step4)]\n",
    "\n",
    "base_esn = SeqToSeqESNClassifier(**initially_fixed_params)\n",
    "\n",
    "try:\n",
    "    sequential_search = load(\"../sequential_search_RICSyN2015.joblib\")\n",
    "except FileNotFoundError:\n",
    "    sequential_search = SequentialSearchCV(base_esn, searches=searches).fit(X_train, y_train)\n",
    "    dump(sequential_search, \"../sequential_search_RICSyN2015.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
