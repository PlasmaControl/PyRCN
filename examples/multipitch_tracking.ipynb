{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multipitch tracking using Echo State Networks\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we demonstrate how the ESN can deal with multipitch tracking, a challenging multilabel classification problem in music analysis.\n",
    "\n",
    "As this is a computationally expensive task, we have pre-trained models to serve as an entry point.\n",
    "\n",
    "At first, we import all packages required for this task. You can find the import statements below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from joblib import dump, load\n",
    "\n",
    "import librosa\n",
    "from madmom.processors import SequentialProcessor, ParallelProcessor\n",
    "from madmom.audio import SignalProcessor, FramedSignalProcessor\n",
    "from madmom.audio.stft import ShortTimeFourierTransformProcessor\n",
    "from madmom.audio.filters import LogarithmicFilterbank\n",
    "from madmom.audio.spectrogram import FilteredSpectrogramProcessor, LogarithmicSpectrogramProcessor, SpectrogramDifferenceProcessor\n",
    "\n",
    "from pyrcn.util import FeatureExtractor\n",
    "from pyrcn.echo_state_network import SeqToSeqESNClassifier\n",
    "from pyrcn.datasets import fetch_maps_piano_dataset\n",
    "from pyrcn.metrics import accuracy_score\n",
    "from pyrcn.model_selection import SequentialSearchCV\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "The acoustic features extracted from the input signal are obtained by filtering short-term spectra (window length 4096 samples and hop size 10 ms) with a bank of triangular filters in the frequency domain with log-spaced frequencies. The frequency range was 30 Hz to 17 000 Hz and we used 12 filters per octave. We used logarithmic magnitudes and added 1 inside the logarithm to ensure a minimum value of 0 for a frame without energy. The first derivative between adjacent frames was added in order to enrich the features by temporal information. Binary labels indicating absent (value 0) or present (value 1) pitches for each frame are assigned to each frame. Note that this task is a multilabel classification. Each MIDI pitch is a separate class, and multiple or no classes can be active at a discrete frame index.\n",
    "\n",
    "For a more detailed description, please have a look in our repository ([https://github.com/TUD-STKS/Automatic-Music-Transcription](https://github.com/TUD-STKS/Automatic-Music-Transcription)) with several detailed examples for music analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_extraction_pipeline(sr=44100, frame_sizes=[1024, 2048, 4096], fps_hz=100.):\n",
    "    audio_loading = Pipeline([(\"load_audio\", FeatureExtractor(librosa.load, sr=sr, mono=True)),\n",
    "                              (\"normalize\", FeatureExtractor(librosa.util.normalize, norm=np.inf))])\n",
    "\n",
    "    sig = SignalProcessor(num_channels=1, sample_rate=sr)\n",
    "    multi = ParallelProcessor([])\n",
    "    for frame_size in frame_sizes:\n",
    "        frames = FramedSignalProcessor(frame_size=frame_size, fps=fps_hz)\n",
    "        stft = ShortTimeFourierTransformProcessor()  # caching FFT window\n",
    "        filt = FilteredSpectrogramProcessor(filterbank=LogarithmicFilterbank, num_bands=12, fmin=30, fmax=17000,\n",
    "                                            norm_filters=True, unique_filters=True)\n",
    "        spec = LogarithmicSpectrogramProcessor(log=np.log10, mul=5, add=1)\n",
    "        diff = SpectrogramDifferenceProcessor(diff_ratio=0.5, positive_diffs=True, stack_diffs=np.hstack)\n",
    "        # process each frame size with spec and diff sequentially\n",
    "        multi.append(SequentialProcessor([frames, stft, filt, spec, diff]))\n",
    "    feature_extractor = FeatureExtractor(SequentialProcessor([sig, multi, np.hstack]))\n",
    "\n",
    "    feature_extraction_pipeline = Pipeline([(\"audio_loading\", audio_loading),\n",
    "                                            (\"feature_extractor\", feature_extractor)])\n",
    "    return feature_extraction_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the dataset\n",
    "\n",
    "This might require a large amount of and memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "feature_extraction_pipeline = create_feature_extraction_pipeline(sr=44100, frame_sizes=[2048], fps_hz=100)\n",
    "# New object -> PyTorch dataloader / Matlab datastore\n",
    "X_train, X_test, y_train, y_test = fetch_maps_piano_dataset(data_origin=\"/projects/p_transcriber/MAPS\", \n",
    "                                                            data_home=None, preprocessor=feature_extraction_pipeline,\n",
    "                                                            force_preprocessing=False, label_type=\"pitch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsplot(ax, data,**kw):\n",
    "    x = np.arange(data.shape[1])\n",
    "    est = np.mean(data, axis=0)\n",
    "    sd = np.std(data, axis=0)\n",
    "    cis = (est - sd, est + sd)\n",
    "    ax.fill_between(x,cis[0],cis[1],alpha=0.2, **kw)\n",
    "    ax.plot(x,est,**kw)\n",
    "    ax.margins(x=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(4, 1.25)\n",
    "tsplot(ax, np.concatenate(np.hstack((X_train, X_test))))\n",
    "ax.set_xlabel('Feature Index')\n",
    "ax.set_ylabel('Magnitude')\n",
    "plt.grid()\n",
    "plt.savefig('features_statistics.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a ESN\n",
    "\n",
    "To develop an ESN model for multipitch tracking, we need to tune several hyper-parameters, e.g., input_scaling, spectral_radius, bias_scaling and leaky integration.\n",
    "\n",
    "We follow the way proposed in the paper for multipitch tracking and for acoustic modeling of piano music to optimize hyper-parameters sequentially.\n",
    "\n",
    "We define the search spaces for each step together with the type of search (a grid search in this context).\n",
    "\n",
    "At last, we initialize a SeqToSeqESNClassifier with the desired output strategy and with the initially fixed parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initially_fixed_params = {'hidden_layer_size': 500,\n",
    "                          'input_activation': 'identity',\n",
    "                          'k_in': 10,\n",
    "                          'bias_scaling': 0.0,\n",
    "                          'reservoir_activation': 'tanh',\n",
    "                          'leakage': 1.0,\n",
    "                          'bi_directional': False,\n",
    "                          'k_rec': 10,\n",
    "                          'wash_out': 0,\n",
    "                          'continuation': False,\n",
    "                          'alpha': 1e-5,\n",
    "                          'random_state': 42}\n",
    "\n",
    "step1_esn_params = {'leakage': np.linspace(0.1, 1.0, 10)}\n",
    "step2_esn_params = {'input_scaling': np.linspace(0.1, 1.0, 10),\n",
    "                    'spectral_radius': np.linspace(0.0, 1.5, 16)}\n",
    "\n",
    "step3_esn_params = {'bias_scaling': np.linspace(0.0, 2.0, 21)}\n",
    "\n",
    "kwargs = {'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "\n",
    "# The searches are defined similarly to the steps of a sklearn.pipeline.Pipeline:\n",
    "searches = [('step1', GridSearchCV, step1_esn_params, kwargs),\n",
    "            ('step2', GridSearchCV, step2_esn_params, kwargs),\n",
    "            ('step3', GridSearchCV, step3_esn_params, kwargs)]\n",
    "\n",
    "base_esn = SeqToSeqESNClassifier(**initially_fixed_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "We provide a SequentialSearchCV that basically iterates through the list of searches that we have defined before. It can be combined with any model selection tool from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    sequential_search = load(\"../sequential_search.joblib\")\n",
    "except FileNotFoundError:\n",
    "    print(FileNotFoundError)\n",
    "    sequential_search = SequentialSearchCV(base_esn, searches=searches).fit(X_train, y_train)\n",
    "    dump(sequential_search, \"sequential_search.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the ESN with final hyper-parameters\n",
    "\n",
    "After the optimization, we extract the ESN with final hyper-parameters as the result of the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_esn = sequential_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step1\"])\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax = sns.lineplot(data=df, x=\"param_leakage\", y=\"mean_test_score\")\n",
    "plt.xlabel(\"Leakage\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlim((0.1, 1))\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.4f'))\n",
    "plt.grid()\n",
    "plt.savefig('optimize_leakage.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step2\"])\n",
    "pvt = pd.pivot_table(df,\n",
    "                     values='mean_test_score', index='param_input_scaling', columns='param_spectral_radius')\n",
    "\n",
    "pvt.columns = pvt.columns.astype(float)\n",
    "pvt2 =  pd.DataFrame(pvt.loc[pd.IndexSlice[0:1], pd.IndexSlice[0.0:1.0]])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = sns.heatmap(pvt2, xticklabels=pvt2.columns.values.round(2), yticklabels=pvt2.index.values.round(2), cbar_kws={'label': 'Score'})\n",
    "ax.invert_yaxis()\n",
    "plt.xlabel(\"Spectral Radius\")\n",
    "plt.ylabel(\"Input Scaling\")\n",
    "fig.set_size_inches(4, 2.5)\n",
    "tick_locator = ticker.MaxNLocator(10)\n",
    "ax.yaxis.set_major_locator(tick_locator)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "plt.savefig('optimize_is_sr.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step3\"])\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax = sns.lineplot(data=df, x=\"param_bias_scaling\", y=\"mean_test_score\")\n",
    "plt.xlabel(\"Bias Scaling\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlim((0, 2))\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.5f'))\n",
    "plt.grid()\n",
    "plt.savefig('optimize_bias_scaling.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the ESN\n",
    "\n",
    "Finally, we increase the reservoir size and compare the impact of uni- and bidirectional ESNs. Notice that the ESN strongly benefit from both, increasing the reservoir size and from the bi-directional working mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'hidden_layer_size': [500, 1000, 2000, 4000, 5000, 8000, 12000, 16000, 20000, 24000, 32000],\n",
    "              'bi_directional': [False, True]}\n",
    "\n",
    "print(\"CV results\\tFit time\\tInference time\\tAccuracy score\\tSize[Bytes]\")\n",
    "for params in ParameterGrid(param_grid):\n",
    "    esn_cv  = GridSearchCV(estimator=clone(base_esn), param_grid=params, scoring=make_scorer(accuracy_score), n_jobs=-1, refit=False).fit(X=X_train, y=y_train)\n",
    "    t1 = time.time()\n",
    "    esn = clone(base_esn).set_params(**params).fit(X_train, y_train, n_jobs=8)\n",
    "    t_fit = time.time() - t1\n",
    "    dump(esn, \"esn_\" + str(params[hidden_layer_size]) + \"_\" + str(params[bi_directional]) + \".joblib\")\n",
    "    mem_size = esn.__sizeof__()\n",
    "    t1 = time.time()\n",
    "    acc_score = accuracy_score(y_test, esn.predict(X_test))\n",
    "    t_inference = time.time() - t1\n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\".format(esn_cv, t_fit, t_inference, acc_score, mem_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
