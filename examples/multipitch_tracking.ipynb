{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multipitch tracking using Echo State Networks\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we demonstrate how the ESN can deal with multipitch tracking, a challenging multilabel classification problem in music analysis.\n",
    "\n",
    "As this is a computational expensive task, we have pre-trained models to serve as an entry point.\n",
    "\n",
    "The notebook depends on several packages: madmom, mir_eval, numpy, matplotlib, IPython and pyrcn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from shutil import copyfile\n",
    "from sklearn.base import clone\n",
    "from joblib import dump, load, Parallel, delayed\n",
    "\n",
    "import soundfile as sf\n",
    "from madmom.processors import SequentialProcessor, ParallelProcessor\n",
    "from madmom.audio import SignalProcessor, FramedSignalProcessor\n",
    "from madmom.audio.stft import ShortTimeFourierTransformProcessor\n",
    "from madmom.audio.filters import LogarithmicFilterbank\n",
    "from madmom.audio.spectrogram import FilteredSpectrogramProcessor, LogarithmicSpectrogramProcessor, SpectrogramDifferenceProcessor\n",
    "\n",
    "from pyrcn.echo_state_network import ESNRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For training and evaluation, we have used the MusicNet dataset, which can be downloaded for free from ([https://homes.cs.washington.edu/~thickstn/musicnet.html](https://homes.cs.washington.edu/~thickstn/musicnet.html)). \n",
    "\n",
    "In the following, we assume that you have downloaded and extracted the dataset successfully to the subfolder \"dataset/MusicNet\".\n",
    "\n",
    "Let us define a function to return the names of all audio files from the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset_path: str = r\"C:\\Users\\Steiner\\Documents\\Python\\PyRCN\\examples\\dataset\\MusicNet\"):\n",
    "    \"\"\"\n",
    "    dataset_path: str: /full/path/to/dataset/\n",
    "    \"\"\"\n",
    "    dataset_path = os.path.normpath(dataset_path)\n",
    "    train_ids = os.listdir(os.path.join(dataset_path, 'train_data'))\n",
    "    test_ids = os.listdir(os.path.join(dataset_path, 'test_data'))\n",
    "    return train_ids, test_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare an ESN model\n",
    "\n",
    "We have already trained ESN models with different reservoir sizes in the uni- and bidirectional mode. \n",
    "\n",
    "In case you would like to train new models, we provide the code here. Note that this will take a lot of time as the MusicNet is a large-scale dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    esn = load(r\"C:\\Users\\Steiner\\Documents\\Python\\PyRCN\\examples\\dataset\\MusicNet\\models\\esn_500_False.joblib\")\n",
    "except:\n",
    "    esn = ESNRegressor(k_in=10, input_scaling=0.2, spectral_radius=0.7, bias=0.7, leakage=0.3, reservoir_size=500, k_res=10,\n",
    "                       reservoir_activation='tanh', bi_directional=False, solver='ridge', beta=1e-4, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "The acoustic features extracted from the input signal are obtained by filtering short-term spectra (window length 4096 samples and hop size 10 ms) with a bank of triangular filters in the frequency domain with log-spaced frequencies. The frequency range was 30 Hz to 17 000 Hz and we used 12 filters per octave. We used logarithmic magnitudes and added 1 inside the logarithm to ensure a minimum value of 0 for a frame without energy. The first derivative between adjacent frames was added in order to enrich the features by temporal information. Binary labels indicating absent (value 0) or present (value 1) pitches for each frame are assigned to each frame. Note that this task is a multilabel classification. Each MIDI pitch is a separate class, and multiple or no classes can be active at a discrete frame index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(in_folder, file_name):\n",
    "    full_path_to_audio = os.path.join(in_folder, file_name)\n",
    "    full_path_to_label = str.replace(full_path_to_audio, 'wav', 'csv').replace('_data', '_labels')\n",
    "    y, sr = sf.read(file=full_path_to_audio)\n",
    "    if y.ndim > 1:\n",
    "        y = y[:, 0]\n",
    "    y = y / np.max(np.abs(y))\n",
    "\n",
    "    sig = SignalProcessor(num_channels=1, sample_rate=sr)\n",
    "    multi = ParallelProcessor([])\n",
    "    frames = FramedSignalProcessor(frame_size=frame_size, fps=100)\n",
    "    stft = ShortTimeFourierTransformProcessor()  # caching FFT window\n",
    "    filt = FilteredSpectrogramProcessor(filterbank=LogarithmicFilterbank, num_bands=12, fmin=30, fmax=17000, norm_filters=True, unique_filters=True)\n",
    "    spec = LogarithmicSpectrogramProcessor(log=np.log10, mul=5, add=1.0)\n",
    "    diff = SpectrogramDifferenceProcessor(diff_ratio=0.5, positive_diffs=True, stack_diffs=np.hstack)\n",
    "    # process each frame size with spec and diff sequentially\n",
    "    multi.append(SequentialProcessor([frames, stft, filt, spec, diff]))  #\n",
    "    pre_processor = SequentialProcessor([sig, multi, np.hstack])\n",
    "\n",
    "    S = pre_processor.process(data=y) - 1.0\n",
    "    \n",
    "    tree = IntervalTree()\n",
    "    with open(fname, 'r') as f:\n",
    "        reader = csv.DictReader(f, delimiter=',')\n",
    "        for label in reader:\n",
    "            start_time = float(label['start_time']) / 44100.\n",
    "            end_time = float(label['end_time']) / 44100.\n",
    "            note = int(label['note'])\n",
    "            tree[start_time:end_time] = (note)\n",
    "\n",
    "    labels = tree\n",
    "    y_notes = np.zeros(shape=(S.shape[0], 128), dtype=int)\n",
    "    for interval in labels.all_intervals:\n",
    "        y_notes[int(100*interval.begin):int(100*interval.end), int(interval.data)] = 1\n",
    "    return S, y_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model\n",
    "\n",
    "This takes a long time. Be very careful here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "should_train = False\n",
    "\n",
    "if should_train:\n",
    "    for fid in train_ids:\n",
    "        X, y_true = extract_features(os.path.join(in_folder, 'train_data'), file_name=fid)\n",
    "        esn.partial_fit(X=X, y=y_true, update_output_weights=False)\n",
    "    esn.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the ESN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = []\n",
    "Y_true = []\n",
    "\n",
    "for fid in train_ids:\n",
    "    X, y_true = extract_features(os.path.join(in_folder, 'test_data'), file_name=fid)\n",
    "    y_pred = esn.predict(X=X)\n",
    "    Y_true.append(y_true)\n",
    "    Y_pred.append(np.asarray(y_pred > 0.3, dtype=int))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
