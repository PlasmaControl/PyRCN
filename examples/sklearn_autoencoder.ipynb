{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder for ESNs using sklearn\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we demonstrate how the ESN can deal with multipitch tracking, a challenging multilabel classification problem in music analysis.\n",
    "\n",
    "As this is a computationally expensive task, we have pre-trained models to serve as an entry point.\n",
    "\n",
    "At first, we import all packages required for this task. You can find the import statements below.\n",
    "\n",
    "To use another objective than `accuracy_score` for hyperparameter tuning, check out the documentation of [make_scorer](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) or ask me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from scipy.stats import uniform\n",
    "from joblib import dump, load\n",
    "\n",
    "from pyrcn.echo_state_network import SeqToSeqESNClassifier  # SeqToSeqESNRegressor or SeqToLabelESNClassifier\n",
    "from pyrcn.metrics import accuracy_score  # more available or create custom score\n",
    "from pyrcn.model_selection import SequentialSearchCV\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "%matplotlib inline\n",
    "#Options\n",
    "plt.rc('image', cmap='RdBu')\n",
    "plt.rc('font', family='serif', serif='Times')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('xtick', labelsize=8)\n",
    "plt.rc('ytick', labelsize=8)\n",
    "plt.rc('axes', labelsize=8)\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png', 'pdf')\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the dataset\n",
    "\n",
    "This might require a large amount of and memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At first, please load all training and test sequences and targets. \n",
    "# Each sequence should be a numpy.array with the shape (n_samples, n_features)\n",
    "# Each target should be\n",
    "# - either be a numpy.array with the shape (n_samples, n_targets) \n",
    "# - or a 1D numpy.array with the shape (n_samples, 1)\n",
    "train_sequences = ......................\n",
    "train_targets = ......................\n",
    "if len(train_sequences) != len(train_targets):\n",
    "    raise ValueError(\"Number of training sequences does not match number of training targets!\")\n",
    "n_train_sequences = len(train_sequences)\n",
    "test_sequences = ......................\n",
    "test_targets = ......................\n",
    "if len(test_sequences) != len(test_targets):\n",
    "    raise ValueError(\"Number of test sequences does not match number of test targets!\")\n",
    "n_test_sequences = len(test_sequences)\n",
    "\n",
    "# Initialize training and test sequences\n",
    "X_train = np.empty(shape=(n_train_sequences, ), dtype=object)\n",
    "y_train = np.empty(shape=(n_train_sequences, ), dtype=object)\n",
    "X_test = np.empty(shape=(n_test_sequences, ), dtype=object)\n",
    "y_test = np.empty(shape=(n_test_sequences, ), dtype=object)\n",
    "\n",
    "for k, (train_sequence, train_target) in enumerate(zip(train_sequences, train_targets)):\n",
    "    X_train[k] = train_sequence\n",
    "    y_train[k] = train_target\n",
    "\n",
    "for k, (test_sequence, test_target) in enumerate(zip(test_sequences, test_targets)):\n",
    "    X_test[k] = test_sequence\n",
    "    y_test[k] = test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial variables to be equal in the Autoencoder and in the ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 500\n",
    "input_activation = 'relu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a MLP autoencoder\n",
    "\n",
    "Currently very rudimentary. However, it can be flexibly made deeper or more complex. Check [MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) documentation for hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_autoencoder = MLPRegressor(hidden_layer_sizes=(hidden_layer_size, ), activation=input_activation)\n",
    "# X_train is a numpy array of sequences - the MLP does not handle sequences. Thus, concatenate all sequences\n",
    "# Target of an autoencoder is the input of the autoencoder\n",
    "mlp_autoencoder.fit(np.concatenate(X_train), np.concatenate(X_train))\n",
    "\n",
    "w_in = np.divide(mlp_autoencoder.coefs_[0], np.linalg.norm(mlp_autoencoder.coefs_[0], axis=0)[None, :])\n",
    "# w_in = mlp_autoencoder.coefs_[0]  # uncomment in case that the vector norm does not make sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up an ESN\n",
    "\n",
    "To develop an ESN model, we need to tune several hyper-parameters, e.g., input_scaling, spectral_radius, bias_scaling and leaky integration.\n",
    "\n",
    "We define the search spaces for each step in a sequential search together with the type of search (a grid or random search in this context).\n",
    "\n",
    "At last, we initialize a SeqToSeqESNClassifier with the desired output strategy and with the initially fixed parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initially_fixed_params = {'hidden_layer_size': hidden_layer_size,\n",
    "                          'k_in': 10,\n",
    "                          'input_scaling': 0.4,\n",
    "                          'input_activation': input_activation,\n",
    "                          'bias_scaling': 0.0,\n",
    "                          'spectral_radius': 0.0,\n",
    "                          'leakage': 1.0,\n",
    "                          'k_rec': 10,\n",
    "                          'reservoir_activation': 'tanh',\n",
    "                          'bi_directional': False,\n",
    "                          'wash_out': 0,\n",
    "                          'continuation': False,\n",
    "                          'alpha': 1e-3,\n",
    "                          'random_state': 42}\n",
    "\n",
    "step1_esn_params = {'input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'spectral_radius': uniform(loc=0, scale=2)}\n",
    "\n",
    "step2_esn_params = {'leakage': loguniform(1e-5, 1e0)}\n",
    "step3_esn_params = {'bias_scaling': np.linspace(0.0, 1.0, 11)}\n",
    "step4_esn_params = {'alpha': loguniform(1e-5, 1e1)}\n",
    "\n",
    "kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "kwargs_step3 = {'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "kwargs_step4 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': make_scorer(accuracy_score)}\n",
    "\n",
    "# The searches are defined similarly to the steps of a sklearn.pipeline.Pipeline:\n",
    "searches = [('step1', RandomizedSearchCV, step1_esn_params, kwargs_step1),\n",
    "            ('step2', RandomizedSearchCV, step2_esn_params, kwargs_step2),\n",
    "            ('step3', GridSearchCV, step3_esn_params, kwargs_step3),\n",
    "            ('step4', RandomizedSearchCV, step4_esn_params, kwargs_step4)]\n",
    "\n",
    "base_esn = SeqToSeqESNClassifier(input_to_node=PredefinedWeightsInputToNode(predefined_input_weights=w_in),\n",
    "                                 **initially_fixed_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "We provide a SequentialSearchCV that basically iterates through the list of searches that we have defined before. It can be combined with any model selection tool from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    sequential_search = load(\"sequential_search.joblib\")\n",
    "except FileNotFoundError:\n",
    "    print(FileNotFoundError)\n",
    "    sequential_search = SequentialSearchCV(base_esn, searches=searches).fit(X_train, y_train)\n",
    "    dump(sequential_search, \"sequential_search.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize hyper-parameter optimization\n",
    "\n",
    "### First optimization step: input scaling and spectral radius\n",
    "\n",
    "Either create a scatterplot - useful in case of a random search to optimize input scaling and spectral radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step1\"])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = sns.scatterplot(x=\"param_spectral_radius\", y=\"param_input_scaling\", hue=\"mean_test_score\", palette='RdBu', data=df)\n",
    "plt.xlabel(\"Spectral Radius\")\n",
    "plt.ylabel(\"Input Scaling\")\n",
    "\n",
    "norm = plt.Normalize(0, df['mean_test_score'].max())\n",
    "sm = plt.cm.ScalarMappable(cmap=\"RdBu\", norm=norm)\n",
    "sm.set_array([])\n",
    "plt.xlim((0, 2.05))\n",
    "plt.ylim((0, 1.05))\n",
    "\n",
    "# Remove the legend and add a colorbar\n",
    "ax.get_legend().remove()\n",
    "ax.figure.colorbar(sm)\n",
    "fig.set_size_inches(4, 2.5)\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.yaxis.set_major_locator(tick_locator)\n",
    "ax.xaxis.set_major_locator(tick_locator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or create a heatmap - useful in case of a grid search to optimize input scaling and spectral radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step1\"])\n",
    "pvt = pd.pivot_table(df,\n",
    "                     values='mean_test_score', index='param_input_scaling', columns='param_spectral_radius')\n",
    "\n",
    "pvt.columns = pvt.columns.astype(float)\n",
    "pvt2 =  pd.DataFrame(pvt.loc[pd.IndexSlice[0:1], pd.IndexSlice[0.0:1.0]])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = sns.heatmap(pvt2, xticklabels=pvt2.columns.values.round(2), yticklabels=pvt2.index.values.round(2), cbar_kws={'label': 'Score'})\n",
    "ax.invert_yaxis()\n",
    "plt.xlabel(\"Spectral Radius\")\n",
    "plt.ylabel(\"Input Scaling\")\n",
    "fig.set_size_inches(4, 2.5)\n",
    "tick_locator = ticker.MaxNLocator(10)\n",
    "ax.yaxis.set_major_locator(tick_locator)\n",
    "ax.xaxis.set_major_locator(tick_locator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second optimization step: leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step2\"])\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax = sns.lineplot(data=df, x=\"param_leakage\", y=\"mean_test_score\")\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel(\"Leakage\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlim((1e-5, 1e0))\n",
    "tick_locator = ticker.MaxNLocator(10)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.4f'))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third optimization step: bias_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step3\"])\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax = sns.lineplot(data=df, x=\"param_bias_scaling\", y=\"mean_test_score\")\n",
    "plt.xlabel(\"Bias Scaling\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlim((0, 1))\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.5f'))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth optimization step: alpha (regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step4\"])\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax = sns.lineplot(data=df, x=\"param_alpha\", y=\"mean_test_score\")\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlim((1e-5, 1e0))\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.5f'))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the ESN\n",
    "\n",
    "Finally, we test the ESN on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = esn.predict(X_train)\n",
    "y_pred_proba = esn.predict_proba(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
