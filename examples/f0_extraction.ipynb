{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f_{0}$ extraction using the Pitch Tracking Dataset from TU Graz (PTDBUG)\n",
    "\n",
    "At first, import packages to be used for the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "cwd = os.getcwd()\n",
    "module_path = os.path.dirname(cwd)  # target working directory\n",
    "\n",
    "sys.path = [item for item in sys.path if item != module_path]  # remove module_path from sys.path\n",
    "sys.path.append(module_path)  # add module_path to sys.path\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from scipy.stats import uniform\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import make_scorer, zero_one_loss\n",
    "from pyrcn.metrics import mean_squared_error\n",
    "from pyrcn.model_selection import SequentialSearchCV\n",
    "from pyrcn.util import FeatureExtractor\n",
    "from pyrcn.datasets import fetch_ptdb_tug_dataset\n",
    "from pyrcn.echo_state_network import ESNRegressor\n",
    "from pyrcn.base.blocks import PredefinedWeightsInputToNode\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "plt.rcParams['text.latex.preamble']=[r\"\\usepackage{lmodern}\"]\n",
    "params = {'text.usetex': True,\n",
    "          'font.size': 8,\n",
    "          'font.family': 'lmodern',\n",
    "         }\n",
    "plt.rcParams.update(params)\n",
    "mpl.rc('font', **{'family': 'serif'})\n",
    "from matplotlib import ticker\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png', 'pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print number of files that are included in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_extraction_pipeline(sr=16000):\n",
    "    audio_loading = Pipeline([(\"load_audio\", FeatureExtractor(func=librosa.load, kw_args={\"sr\": sr, \"mono\": True})),\n",
    "                              (\"normalize\", FeatureExtractor(func=librosa.util.normalize, kw_args={\"norm\": np.inf}))])\n",
    "    \n",
    "    feature_extractor = Pipeline([(\"mel_spectrogram\", FeatureExtractor(func=librosa.feature.melspectrogram, \n",
    "                                                                       kw_args={\"sr\": sr, \"n_fft\": 1024, \"hop_length\": 160, \n",
    "                                                                                \"window\": 'hann', \"center\": False, \n",
    "                                                                                \"power\": 2.0, \"n_mels\": 80, \"fmin\": 40, \n",
    "                                                                                \"fmax\": 4000, \"htk\": True})),\n",
    "                                            (\"power_to_db\", FeatureExtractor(func=librosa.power_to_db, kw_args={\"ref\": 1}))])\n",
    "\n",
    "    feature_extraction_pipeline = Pipeline([(\"audio_loading\", audio_loading),\n",
    "                                            (\"feature_extractor\", feature_extractor)])\n",
    "    return feature_extraction_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the dataset\n",
    "\n",
    "This might require a large amount of time and memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "feature_extraction_pipeline = create_feature_extraction_pipeline()\n",
    "\n",
    "X_train, X_test, y_train, y_test = fetch_ptdb_tug_dataset(data_origin=\"Z:/Projekt-Pitch-Datenbank/SPEECH_DATA\", \n",
    "                                                          data_home=None, preprocessor=feature_extraction_pipeline, \n",
    "                                                          force_preprocessing=False, augment=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature visualization. All features carry information, since the variance is always large\n",
    "\n",
    "We can fit a StandardScaler here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsplot(ax, data,**kw):\n",
    "    x = np.arange(data.shape[1])\n",
    "    est = np.mean(data, axis=0)\n",
    "    sd = np.std(data, axis=0)\n",
    "    cis = (est - sd, est + sd)\n",
    "    ax.fill_between(x,cis[0],cis[1],alpha=0.2, **kw)\n",
    "    ax.plot(x,est,**kw)\n",
    "    ax.margins(x=0)\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, sharex=True)\n",
    "fig.set_size_inches(3.2, 2)\n",
    "tsplot(axs[0], np.concatenate(np.hstack((X_train, X_test))))\n",
    "axs[0].set_ylabel('Magnitude')\n",
    "scaler = StandardScaler().fit(np.concatenate(X_train))\n",
    "for k, X in enumerate(X_train):\n",
    "    X_train[k] = scaler.transform(X=X)\n",
    "for k, X in enumerate(X_test):\n",
    "    X_test[k] = scaler.transform(X=X)\n",
    "tsplot(axs[1], np.concatenate(np.hstack((X_train, X_test))))\n",
    "axs[1].set_xlabel('Feature Index')\n",
    "axs[1].set_ylabel('Magnitude')\n",
    "plt.grid()\n",
    "# plt.savefig('features_statistics.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_y_train = []\n",
    "for k in range(0, 1):\n",
    "    print(k)\n",
    "    all_y_train.append(np.vstack((np.concatenate(y_train), np.concatenate(y_test)))[:, 0] * 2**(k/12))\n",
    "all_y_test = []\n",
    "for k in range(0, 1):\n",
    "    print(k)\n",
    "    all_y_test.append(np.concatenate(y_test)[:, 0] * 2**(k/12))\n",
    "\n",
    "all_y_train = np.concatenate(all_y_train)\n",
    "all_y_test = np.concatenate(all_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(all_y_test[all_y_test!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(3.2, 1.3)\n",
    "sns.histplot(x=all_y_train[all_y_train!=0], stat=\"count\", label=\"All data\", ax=ax)  # , log_scale=True\n",
    "sns.histplot(x=all_y_test[all_y_test!=0], stat=\"count\", label=\"Test set\", color=\"red\", ax=ax)  # , log_scale=True\n",
    "plt.xlabel(r\"$f_{0}$ in Hertz\", fontsize=8)\n",
    "plt.ylabel(r\"Count\", fontsize=8)\n",
    "ax.xaxis.set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "# ax.set_xticks([50, 100, 200, 400])\n",
    "ax.set_xlim([50, 300])\n",
    "ax.set_ylim([0, 55000])\n",
    "ax.tick_params(labelsize=8)\n",
    "plt.minorticks_off()\n",
    "plt.legend(prop={'size': 8})\n",
    "plt.savefig('ptdb_tug_ground_truth.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define several error functions for $f_{0}$ extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpe(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Gross pitch error:\n",
    "    \n",
    "    All frames that are considered voiced by both pitch tracker and ground truth, \n",
    "    for which the relative pitch error is higher than a certain threshold (\\SI{20}{\\percent}).\n",
    "    \n",
    "    \"\"\"\n",
    "    idx = np.nonzero(y_true*y_pred)[0]\n",
    "    return np.sum(np.abs(y_true[idx] - y_pred[idx]) > 0.2 * y_true[idx]) / len(np.nonzero(y_true)[0])\n",
    "\n",
    "\n",
    "def vde(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Voicing Decision Error:\n",
    "    \n",
    "    Proportion of frames for which an incorrect voiced/unvoiced decision is made.\n",
    "    \n",
    "    \"\"\"\n",
    "    return zero_one_loss(y_true, y_pred)\n",
    "\n",
    "\n",
    "def fpe(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Fine Pitch Error:\n",
    "    \n",
    "    Standard deviation of the distribution of relative error values (in cents) from the frames\n",
    "    that do not have gross pitch errors\n",
    "    \"\"\"\n",
    "    idx_voiced = np.nonzero(y_true * y_pred)[0]\n",
    "    idx_correct = np.argwhere(np.abs(y_true - y_pred) <= 0.2 * y_true).ravel()\n",
    "    idx = np.intersect1d(idx_voiced, idx_correct)\n",
    "    if idx.size == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 100 * np.std(np.log2(y_pred[idx] / y_true[idx]))\n",
    "\n",
    "\n",
    "def ffe(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    $f_{0}$ Frame Error:\n",
    "    \n",
    "    Proportion of frames for which an error (either according to the GPE or the VDE criterion) is made.\n",
    "    FFE can be seen as a single measure for assessing the overall performance of a pitch tracker.\n",
    "    \"\"\"\n",
    "    idx_correct = np.argwhere(np.abs(y_true - y_pred) <= 0.2 * y_true).ravel()\n",
    "    return 1 - len(idx_correct) / len(y_true)\n",
    "\n",
    "\n",
    "def custom_scorer(y_true, y_pred):\n",
    "    gross_pitch_error = [None] * len(y_true)\n",
    "    for k, (y_t, y_p) in enumerate(zip(y_true, y_pred)):\n",
    "        gross_pitch_error[k] = gpe(y_true=y_t[:, 0]*y_t[:, 1], y_pred=y_p[:, 0]*(y_p[:, 1] >= .5))\n",
    "    return np.mean(gross_pitch_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpe_scorer = make_scorer(custom_scorer, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a ESN\n",
    "\n",
    "To develop an ESN model for multipitch tracking, we need to tune several hyper-parameters, e.g., input_scaling, spectral_radius, bias_scaling and leaky integration.\n",
    "\n",
    "We follow the way proposed in the paper for multipitch tracking and for acoustic modeling of piano music to optimize hyper-parameters sequentially.\n",
    "\n",
    "We define the search spaces for each step together with the type of search (a grid search in this context).\n",
    "\n",
    "At last, we initialize an```ESNRegressor```ESNRegressor with the desired output strategy and with the initially fixed parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initially_fixed_params = {'hidden_layer_size': 50,\n",
    "                          'input_activation': 'identity',\n",
    "                          'input_scaling': 0.4,\n",
    "                          'k_in': 5,\n",
    "                          'bias_scaling': 0.0,\n",
    "                          'reservoir_activation': 'tanh',\n",
    "                          'spectral_radius': 0.0,\n",
    "                          'leakage': 1.0,\n",
    "                          'bidirectional': False,\n",
    "                          'k_rec': 10,\n",
    "                          'wash_out': 0,\n",
    "                          'continuation': False,\n",
    "                          'alpha': 1e-3,\n",
    "                          'random_state': 42}\n",
    "\n",
    "step1_esn_params = {'input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'spectral_radius': uniform(loc=0, scale=2)}\n",
    "\n",
    "step2_esn_params = {'leakage': loguniform(1e-5, 1e0)}\n",
    "step3_esn_params = {'bias_scaling': np.linspace(0.0, 1.0, 11)}\n",
    "step4_esn_params = {'alpha': loguniform(1e-5, 1e1)}\n",
    "\n",
    "kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': gpe_scorer}\n",
    "kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': gpe_scorer}\n",
    "kwargs_step3 = {'verbose': 1, 'n_jobs': -1, 'scoring': gpe_scorer}\n",
    "kwargs_step4 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': gpe_scorer}\n",
    "\n",
    "searches = [('step1', RandomizedSearchCV, step1_esn_params, kwargs_step1),\n",
    "            ('step2', RandomizedSearchCV, step2_esn_params, kwargs_step2),\n",
    "            ('step3', GridSearchCV, step3_esn_params, kwargs_step3),\n",
    "            ('step4', RandomizedSearchCV, step4_esn_params, kwargs_step4)]\n",
    "\n",
    "base_esn = ESNRegressor(**initially_fixed_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "We provide a SequentialSearchCV that basically iterates through the list of searches that we have defined before. It can be combined with any model selection tool from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    sequential_search = load(\"../f0/sequential_search_f0_mel_50.joblib\")\n",
    "except FileNotFoundError:\n",
    "    print(FileNotFoundError)\n",
    "    sequential_search = SequentialSearchCV(base_esn, searches=searches).fit(X_train, y_train)\n",
    "    dump(sequential_search, \"../f0/sequential_search_f0_mel_50.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize hyper-parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step1\"])\n",
    "df[\"mean_test_score\"] = -df[\"mean_test_score\"]\n",
    "# df.mean_test_score = df.mean_test_score.clip(upper=1.5e-1)\n",
    "fig = plt.figure()\n",
    "ax = sns.scatterplot(x=\"param_spectral_radius\", y=\"param_input_scaling\", hue=\"mean_test_score\", palette='viridis', data=df)\n",
    "plt.xlabel(\"Spectral Radius\")\n",
    "plt.ylabel(\"Input Scaling\")\n",
    "\n",
    "norm = plt.Normalize(-0.8, -0.1)\n",
    "# sm = plt.cm.ScalarMappable(cmap=\"viridis\")\n",
    "sm = plt.cm.ScalarMappable(cmap=\"viridis\", norm=norm)\n",
    "sm.set_array([])\n",
    "plt.xlim((0, 2.01))\n",
    "plt.ylim((0, 1.03))\n",
    "\n",
    "# Remove the legend and add a colorbar\n",
    "ax.get_legend().remove()\n",
    "ax.figure.colorbar(sm, label='GPE')\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax.yaxis.set_major_locator(ticker.MaxNLocator(5))\n",
    "ax.xaxis.set_major_locator(ticker.MaxNLocator(5))\n",
    "plt.grid()\n",
    "# plt.savefig('optimize_is_sr_50.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step2\"])\n",
    "df[\"mean_test_score\"] = -df[\"mean_test_score\"]\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax = sns.lineplot(data=df, x=\"param_leakage\", y=\"mean_test_score\")\n",
    "plt.xlabel(\"Leakage\")\n",
    "plt.ylabel(\"GPE\")\n",
    "# plt.xlim((0, 1))\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.4f'))\n",
    "plt.grid()\n",
    "# plt.savefig('optimize_leakage_50.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step3\"])\n",
    "df[\"mean_test_score\"] = -df[\"mean_test_score\"]\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax = sns.lineplot(data=df, x=\"param_bias_scaling\", y=\"mean_test_score\")\n",
    "plt.xlabel(\"Bias Scaling\")\n",
    "plt.ylabel(\"GPE\")\n",
    "plt.xlim((0, 1))\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.5f'))\n",
    "plt.grid()\n",
    "# plt.savefig('optimize_bias_scaling_50.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step4\"])\n",
    "df[\"mean_test_score\"] = -df[\"mean_test_score\"]\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax = sns.lineplot(data=df, x=\"param_alpha\", y=\"mean_test_score\")\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"GPE\")\n",
    "plt.xlim((1e-5, 10))\n",
    "ax.set(xscale='log')\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.5f'))\n",
    "plt.grid()\n",
    "# plt.savefig('optimize_alpha_50.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final evaluation\n",
    "\n",
    "Increase the reservoir size from 50 neurons as large as possible by doubling the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    gs = load(\"../f0/sequential_search_f0_mel_50_final.joblib\")\n",
    "except FileNotFoundError:\n",
    "    param_grid = {'hidden_layer_size': [50, 100, 200, 400, 800, 8000, 16000],\n",
    "                  'random_state': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "    gs = GridSearchCV(clone(sequential_search.best_estimator_), param_grid, \n",
    "                      scoring=gpe_scorer, n_jobs=-1, refit=False, verbose=10).fit(X_train, y_train)\n",
    "    dump(gs, \"../f0/sequential_search_f0_mel_50_final.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(gs.cv_results_)\n",
    "df[\"mean_test_score\"] = -df[\"mean_test_score\"]\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(3.2, 1.5)\n",
    "ax = sns.boxplot(data=df, x=\"param_hidden_layer_size\", y=\"mean_test_score\")\n",
    "plt.xlabel(r\"Hidden Layer Size\", fontsize=8)\n",
    "plt.ylabel(r\"GPE\", fontsize=8)\n",
    "# ax.set(xscale='linear')\n",
    "# ax.set(xlim=(0, 16500))\n",
    "ax.set(ylim=(0.2, 0.6))\n",
    "tick_locator = ticker.MaxNLocator(10)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "tick_locator = ticker.MaxNLocator(4)\n",
    "ax.yaxis.set_major_locator(tick_locator)\n",
    "ax.set_yticklabels(ax.get_yticks(), size = 8)\n",
    "ax.tick_params(labelsize=8)\n",
    "ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "plt.xticks(rotation=90)\n",
    "# plt.grid()\n",
    "plt.savefig('esn_final_validation.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$K$-Means initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpe_scorer(sequential_search.best_estimator_, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Attempting to load KMeans from disk...\")\n",
    "    kmeans = load(\"../f0/kmeans_50.joblib\")\n",
    "    print(\"Loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Fitting kmeans with features from the training set...\")\n",
    "    t1 = time.time()\n",
    "    kmeans = MiniBatchKMeans(n_clusters=50, n_init=200, reassignment_ratio=0, max_no_improvement=50, init='k-means++', verbose=2, random_state=0)\n",
    "    kmeans.fit(X=np.concatenate(np.concatenate((X_train, X_test))))\n",
    "    dump(kmeans, \"../f0/kmeans_50.joblib\")\n",
    "    print(\"done in {0}!\".format(time.time() - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize an Echo State Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initially_fixed_params = {'hidden_layer_size': 50,\n",
    "                          'k_in': 10,\n",
    "                          'input_scaling': 0.4,\n",
    "                          'input_activation': 'identity',\n",
    "                          'bias_scaling': 0.0,\n",
    "                          'spectral_radius': 0.0,\n",
    "                          'leakage':1.0,\n",
    "                          'k_rec': 10,\n",
    "                          'reservoir_activation': 'tanh',\n",
    "                          'bidirectional': False,\n",
    "                          'wash_out': 0,\n",
    "                          'continuation': False,\n",
    "                          'alpha': 1e-3,\n",
    "                          'random_state': 42}\n",
    "\n",
    "if initially_fixed_params[\"hidden_layer_size\"] <= 200:\n",
    "    w_in = np.divide(kmeans.cluster_centers_, np.linalg.norm(kmeans.cluster_centers_, axis=1)[:, None])\n",
    "else:\n",
    "    w_in = np.pad(np.divide(kmeans.cluster_centers_, np.linalg.norm(kmeans.cluster_centers_, axis=1)[:, None]), ((0, initially_fixed_params[\"hidden_layer_size\"] - 200), (0, 0)), mode='constant', constant_values=0)\n",
    "\n",
    "base_input_to_node = PredefinedWeightsInputToNode(predefined_input_weights=w_in.T, input_scaling=0.4)\n",
    "\n",
    "step1_esn_params = {'input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'spectral_radius': uniform(loc=0, scale=2)}\n",
    "\n",
    "step2_esn_params = {'leakage': loguniform(1e-5, 1e0)}\n",
    "step3_esn_params = {'bias_scaling': np.linspace(0.0, 1.0, 11)}\n",
    "step4_esn_params = {'alpha': loguniform(1e-5, 1e1)}\n",
    "\n",
    "kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': gpe_scorer}\n",
    "kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': gpe_scorer}\n",
    "kwargs_step3 = {'verbose': 1, 'n_jobs': -1, 'scoring': gpe_scorer}\n",
    "kwargs_step4 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': gpe_scorer}\n",
    "\n",
    "# The searches are defined similarly to the steps of a sklearn.pipeline.Pipeline:\n",
    "searches = [('step1', RandomizedSearchCV, step1_esn_params, kwargs_step1),\n",
    "            ('step2', RandomizedSearchCV, step2_esn_params, kwargs_step2),\n",
    "            ('step3', GridSearchCV, step3_esn_params, kwargs_step3),\n",
    "            ('step4', RandomizedSearchCV, step4_esn_params, kwargs_step4)]\n",
    "\n",
    "base_esn = ESNRegressor(input_to_node=base_input_to_node).set_params(**initially_fixed_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to load a pre-trained ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    sequential_search = load(\"../f0/sequential_search_f0_mel_km_500_sparse.joblib\")\n",
    "except FileNotFoundError:\n",
    "    print(FileNotFoundError)\n",
    "    sequential_search = SequentialSearchCV(base_esn, searches=searches).fit(X_train, y_train)\n",
    "    dump(sequential_search, \"../f0/sequential_search_f0_mel_km_500_sparse.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize hyper-parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step1\"])\n",
    "df[\"mean_test_score\"] = -df[\"mean_test_score\"]\n",
    "# df.mean_test_score = df.mean_test_score.clip(upper=1.5e-1)\n",
    "fig = plt.figure()\n",
    "ax = sns.scatterplot(x=\"param_spectral_radius\", y=\"param_input_scaling\", hue=\"mean_test_score\", palette='viridis', data=df)\n",
    "plt.xlabel(\"Spectral Radius\")\n",
    "plt.ylabel(\"Input Scaling\")\n",
    "\n",
    "norm = plt.Normalize(-0.8, -0.1)\n",
    "# sm = plt.cm.ScalarMappable(cmap=\"viridis\")\n",
    "sm = plt.cm.ScalarMappable(cmap=\"viridis\", norm=norm)\n",
    "sm.set_array([])\n",
    "plt.xlim((0, 2.01))\n",
    "plt.ylim((0, 1.03))\n",
    "\n",
    "# Remove the legend and add a colorbar\n",
    "ax.get_legend().remove()\n",
    "ax.figure.colorbar(sm, label='GPE')\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax.yaxis.set_major_locator(ticker.MaxNLocator(5))\n",
    "ax.xaxis.set_major_locator(ticker.MaxNLocator(5))\n",
    "plt.grid()\n",
    "plt.savefig('optimize_is_sr_km_50.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step2\"])\n",
    "df[\"mean_test_score\"] = -df[\"mean_test_score\"]\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax = sns.lineplot(data=df, x=\"param_leakage\", y=\"mean_test_score\")\n",
    "plt.xlabel(\"Leakage\")\n",
    "plt.ylabel(\"GPE\")\n",
    "# plt.xlim((0, 1))\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.4f'))\n",
    "plt.grid()\n",
    "plt.savefig('optimize_leakage_km_50.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step3\"])\n",
    "df[\"mean_test_score\"] = -df[\"mean_test_score\"]\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax = sns.lineplot(data=df, x=\"param_bias_scaling\", y=\"mean_test_score\")\n",
    "plt.xlabel(\"Bias Scaling\")\n",
    "plt.ylabel(\"GPE\")\n",
    "plt.xlim((0, 1))\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.5f'))\n",
    "plt.grid()\n",
    "plt.savefig('optimize_bias_scaling_km_50.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step4\"])\n",
    "df[\"mean_test_score\"] = -df[\"mean_test_score\"]\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax = sns.lineplot(data=df, x=\"param_alpha\", y=\"mean_test_score\")\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"GPE\")\n",
    "plt.xlim((1e-5, 10))\n",
    "ax.set(xscale='log')\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.5f'))\n",
    "plt.grid()\n",
    "plt.savefig('optimize_alpha_km_50.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final evaluation\n",
    "\n",
    "Increase the reservoir size from 50 neurons as large as possible by doubling the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    gs = load(\"../f0/sequential_search_f0_mel_km_50_final.joblib\")\n",
    "except:\n",
    "    param_grid = {'hidden_layer_size': [50, 100, 200, 400, 800, 1600, 3200, 6400, 8000],  # TODO 16000\n",
    "                  'random_state': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "    gs = []\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        try:\n",
    "            print(\"Attempting to load KMeans from disk...\")\n",
    "            kmeans = load(\"../f0/kmeans_\" + str(params[\"hidden_layer_size\"]) + \".joblib\")\n",
    "            print(\"Loaded.\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Fitting kmeans with features from the training set...\")\n",
    "            t1 = time.time()\n",
    "            kmeans = MiniBatchKMeans(n_clusters=params[\"hidden_layer_size\"], n_init=200, reassignment_ratio=0, max_no_improvement=50, init='k-means++', verbose=0, random_state=0)\n",
    "            kmeans.fit(X=np.concatenate(np.concatenate((X_train, X_test))))\n",
    "            dump(kmeans, \"f0/kmeans_\" + str(params[\"hidden_layer_size\"]) + \".joblib\")\n",
    "            print(\"done in {0}!\".format(time.time() - t1))\n",
    "        w_in = np.divide(kmeans.cluster_centers_, np.linalg.norm(kmeans.cluster_centers_, axis=1)[:, None])\n",
    "        base_input_to_node = PredefinedWeightsInputToNode(predefined_input_weights=w_in.T)\n",
    "        esn = clone(sequential_search.best_estimator_)\n",
    "        esn.input_to_node.predefined_input_weights=w_in.T\n",
    "        esn.set_params(**params)\n",
    "        gs.append(cross_validate(esn, X=X_train, y=y_train, n_jobs=-1, scoring=gpe_scorer, verbose=10))\n",
    "    dump(gs, \"../f0/sequential_search_f0_mel_km_50_final.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(gs.cv_results_)\n",
    "df[\"mean_test_score\"] = -df[\"mean_test_score\"]\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(3.2, 1.5)\n",
    "ax = sns.boxplot(data=df, x=\"param_hidden_layer_size\", y=\"mean_test_score\")\n",
    "plt.xlabel(r\"Hidden Layer Size\", fontsize=8)\n",
    "plt.ylabel(r\"GPE\", fontsize=8)\n",
    "# ax.set(xscale='linear')\n",
    "# ax.set(xlim=(0, 16500))\n",
    "ax.set(ylim=(0.2, 0.6))\n",
    "tick_locator = ticker.MaxNLocator(10)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "tick_locator = ticker.MaxNLocator(4)\n",
    "ax.yaxis.set_major_locator(tick_locator)\n",
    "ax.set_yticklabels(ax.get_yticks(), size = 8)\n",
    "ax.tick_params(labelsize=8)\n",
    "ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "plt.xticks(rotation=90)\n",
    "# plt.grid()\n",
    "plt.savefig('dense_km_esn_final_validation.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_search.best_estimator_.node_to_node.n_features_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list the files\n",
    "path = os.path.abspath(r\"C:\\Users\\Steiner\\Documents\\Python\\PyRCN\\f0\\PTDB_TUG\")\n",
    "filelist = os.listdir(path) \n",
    "#read them into pandas\n",
    "df_list = [pd.read_csv(os.path.join(path, f), sep=\" \", skiprows=lambda idx: idx < 7, header=None) for f in filelist]\n",
    "#concatenate them together\n",
    "reaper_df = pd.concat(df_list)\n",
    "reaper_df.columns = [\"time\", \"Voicing\", \"f0\"]\n",
    "all_y_reaper = reaper_df[[\"f0\"]].to_numpy()\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(2.5, 1.2)\n",
    "ax = sns.histplot(x=all_y_reaper[all_y_reaper != -1], stat=\"count\", legend=False)  # , log_scale=True\n",
    "plt.xlabel(r\"$f_{0}$ in Hertz\", fontsize=8)\n",
    "plt.ylabel(r\"Count\", fontsize=8)\n",
    "ax.xaxis.set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "# ax.set_xticks([50, 100, 200, 400])\n",
    "ax.set_xlim([50, 300])\n",
    "ax.set_ylim([0, 55000])\n",
    "ax.tick_params(labelsize=8)\n",
    "plt.minorticks_off()\n",
    "plt.savefig('ptdb_tug_reaper.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list the files\n",
    "path = os.path.abspath(r\"C:\\Users\\Steiner\\Documents\\Python\\PyRCN\\f0\\VCTK\")\n",
    "filelist = os.listdir(path) \n",
    "#read them into pandas\n",
    "df_list = [pd.read_csv(os.path.join(path, f), sep=\" \", skiprows=lambda idx: idx < 7, header=None) for f in filelist]\n",
    "#concatenate them together\n",
    "reaper_df = pd.concat(df_list)\n",
    "reaper_df.columns = [\"time\", \"Voicing\", \"f0\"]\n",
    "all_y_reaper = reaper_df[[\"f0\"]].to_numpy()\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(4, 2.5)\n",
    "ax = sns.histplot(x=all_y_reaper[all_y_reaper != -1], stat=\"count\")  # , log_scale=True\n",
    "ax.set(xlabel=r'$f_{0}$ in Hertz', ylabel='Count')\n",
    "ax.xaxis.set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "# ax.set_xticks([50, 100, 200, 400])\n",
    "ax.set_xlim([40, 400])\n",
    "plt.minorticks_off()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpe_scorer(y_true, y_pred):\n",
    "    gross_pitch_error = [None] * len(y_true)\n",
    "    for k, (y_t, y_p) in enumerate(zip(y_true, y_pred)):\n",
    "        gross_pitch_error[k] = gpe(y_true=y_t[:, 0]*(y_t[:, 1] > 0.5), y_pred=y_p[:, 0]*(y_p[:, 1] >= .5))\n",
    "    return np.mean(gross_pitch_error)\n",
    "\n",
    "def fpe_scorer(y_true, y_pred):\n",
    "    fine_pitch_error = [None] * len(y_true)\n",
    "    for k, (y_t, y_p) in enumerate(zip(y_true, y_pred)):\n",
    "        fine_pitch_error[k] = fpe(y_true=y_t[:, 0]*(y_t[:, 1] > 0.5), y_pred=y_p[:, 0]*(y_p[:, 1] >= .5))\n",
    "    return np.mean(fine_pitch_error)\n",
    "\n",
    "def vde_scorer(y_true, y_pred):\n",
    "    voicing_decision_error = [None] * len(y_true)\n",
    "    for k, (y_t, y_p) in enumerate(zip(y_true, y_pred)):\n",
    "        voicing_decision_error[k] = vde(y_true=(y_t[:, 1] > 0.5), y_pred=y_p[:, 1]>=.5)\n",
    "    return np.mean(voicing_decision_error)\n",
    "\n",
    "def ffe_scorer(y_true, y_pred):\n",
    "    frame_fault_error = [None] * len(y_true)\n",
    "    for k, (y_t, y_p) in enumerate(zip(y_true, y_pred)):\n",
    "        frame_fault_error[k] = ffe(y_true=y_t[:, 0]*(y_t[:, 1] > 0.5), y_pred=y_p[:, 0]*(y_p[:, 1] >= .5))\n",
    "    return np.mean(frame_fault_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'hidden_layer_size': [50,100,200,400,500,800,1000,1600,2000,3200,4000,6400,8000,16000],\n",
    "              'k': [0,1,2,3,4,5,6]}\n",
    "\n",
    "print(\"hidden_layer_size,supervised_st,unsupervised_st,GPE,FPE,VDE,FFE\")\n",
    "for params in ParameterGrid(param_grid):\n",
    "    esn = load(\"../f0/esn_\" + str(params[\"hidden_layer_size\"]) + \"_\" + str(params[\"k\"]) + \".joblib\")\n",
    "    y_pred = esn.predict(X_test)\n",
    "    print(\"{0},{1},{2},{3},{4},{5},{6}\".format(params[\"hidden_layer_size\"],params[\"k\"],params[\"k\"],\n",
    "                                               gpe_scorer(y_test, y_pred),\n",
    "                                               fpe_scorer(y_test, y_pred),\n",
    "                                               vde_scorer(y_test, y_pred),\n",
    "                                               ffe_scorer(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'hidden_layer_size': [50,100,200,400,500,800,1000,1600,2000,3200,4000,6400,8000,16000],\n",
    "              'k': [0,1,2,3,4,5,6]}\n",
    "\n",
    "print(\"hidden_layer_size,supervised_st,unsupervised_st,GPE,FPE,VDE,FFE\")\n",
    "for params in ParameterGrid(param_grid):\n",
    "    esn = load(\"../f0/km_esn_dense_\" + str(params[\"hidden_layer_size\"]) + \"_0_\" + str(params[\"k\"]) + \".joblib\")\n",
    "    y_pred = esn.predict(X_test)\n",
    "    print(\"{0},{1},{2},{3},{4},{5},{6}\".format(params[\"hidden_layer_size\"],params[\"k\"],params[\"k\"],\n",
    "                                               gpe_scorer(y_test, y_pred),\n",
    "                                               fpe_scorer(y_test, y_pred),\n",
    "                                               vde_scorer(y_test, y_pred),\n",
    "                                               ffe_scorer(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'hidden_layer_size': [50,100,200,400,500,800,1000,1600,2000,3200,4000,6400,8000,16000],\n",
    "              'k': [0,1,2,3,4,5,6]}\n",
    "\n",
    "print(\"hidden_layer_size,supervised_st,unsupervised_st,GPE,FPE,VDE,FFE\")\n",
    "for params in ParameterGrid(param_grid):\n",
    "    esn = load(\"../f0/km_esn_dense_\" + str(params[\"hidden_layer_size\"]) + \"_\" + str(params[\"k\"]) + \"_0.joblib\")\n",
    "    y_pred = esn.predict(X_test)\n",
    "    print(\"{0},{1},{2},{3},{4},{5},{6}\".format(params[\"hidden_layer_size\"],params[\"k\"],params[\"k\"],\n",
    "                                               gpe_scorer(y_test, y_pred),\n",
    "                                               fpe_scorer(y_test, y_pred),\n",
    "                                               vde_scorer(y_test, y_pred),\n",
    "                                               ffe_scorer(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'hidden_layer_size': [50,100,200,400,500,800,1000,1600,2000,3200,4000,6400,8000,16000],\n",
    "              'k': [0,1,2,3,4,5,6]}\n",
    "\n",
    "print(\"hidden_layer_size,supervised_st,unsupervised_st,GPE,FPE,VDE,FFE\")\n",
    "for params in ParameterGrid(param_grid):\n",
    "    esn = load(\"../f0/km_esn_dense_\" + str(params[\"hidden_layer_size\"]) + \"_\" + str(params[\"k\"]) + \"_\" + str(params[\"k\"]) + \".joblib\")\n",
    "    y_pred = esn.predict(X_test)\n",
    "    print(\"{0},{1},{2},{3},{4},{5},{6}\".format(params[\"hidden_layer_size\"],params[\"k\"],params[\"k\"],\n",
    "                                               gpe_scorer(y_test, y_pred),\n",
    "                                               fpe_scorer(y_test, y_pred),\n",
    "                                               vde_scorer(y_test, y_pred),\n",
    "                                               ffe_scorer(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"../f0/basic_esn_final.csv\", sep=\",\")\n",
    "df2 = pd.read_csv(\"../f0/km_esn_only_supervised.csv\", sep=\",\")\n",
    "df3 = pd.read_csv(\"../f0/km_esn_only_unsupervised.csv\", sep=\",\")\n",
    "df4 = pd.read_csv(\"../f0/km_esn_completely_augmented.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1[\"GPE\"] = 100*df1[\"GPE\"]\n",
    "df2[\"GPE\"] = 100*df2[\"GPE\"]\n",
    "df3[\"GPE\"] = 100*df3[\"GPE\"]\n",
    "df4[\"GPE\"] = 100*df4[\"GPE\"]\n",
    "df1[\"VDE\"] = 100*df1[\"VDE\"]\n",
    "df2[\"VDE\"] = 100*df2[\"VDE\"]\n",
    "df3[\"VDE\"] = 100*df3[\"VDE\"]\n",
    "df4[\"VDE\"] = 100*df4[\"VDE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(3.2, 1.3)\n",
    "sns.lineplot(data=df1[df1['supervised\\_st'] == 0], x=\"hidden\\_layer\\_size\", y=\"GPE\", marker=\"o\", ax=ax)\n",
    "sns.lineplot(data=df2[df2['supervised\\_st'] == 0], x=\"hidden\\_layer\\_size\", y=\"GPE\", marker=\"o\", ax=ax)\n",
    "ax.set_xlim([0, 16200])\n",
    "ax.tick_params(labelsize=8)\n",
    "plt.legend(labels=[\"Basic ESN\",\"KM-ESN\"],prop={'size': 8})\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "plt.xlabel(\"Reservoir size\", fontsize=8)\n",
    "plt.ylabel(\"GPE in \\%\", fontsize=8)\n",
    "# ax.set_ylim([0, 22000])\n",
    "plt.savefig('GPE_test.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(3.2, 1.3)\n",
    "sns.lineplot(data=df1[df1['supervised\\_st'] == 0], x=\"hidden\\_layer\\_size\", y=\"VDE\", marker=\"o\", ax=ax)\n",
    "sns.lineplot(data=df2[df2['supervised\\_st'] == 0], x=\"hidden\\_layer\\_size\", y=\"VDE\", marker=\"o\", ax=ax)\n",
    "ax.set_xlim([0, 16200])\n",
    "ax.tick_params(labelsize=8)\n",
    "plt.legend(labels=[\"Basic ESN\",\"KM-ESN\"],prop={'size': 8})\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "plt.xlabel(\"Reservoir size\", fontsize=8)\n",
    "plt.ylabel(\"VDE in \\%\", fontsize=8)\n",
    "# ax.set_ylim([0, 22000])\n",
    "plt.savefig('VDE_test.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['legend.title_fontsize'] = 8\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(3.2, 1.3)\n",
    "sns.lineplot(data=df1, x=\"hidden\\_layer\\_size\", y=\"GPE\", hue=\"supervised\\_st\", marker=\"o\", ax=ax)\n",
    "ax.set_xlim([0, 16200])\n",
    "ax.tick_params(labelsize=8)\n",
    "l = plt.legend([\"0 st\", \"1 st\", \"2 st\", \"3 st\", \"4 st\", \"5 st\", \"6 st\"], \n",
    "               title=\"Pitch shift\\n up to\", \n",
    "               prop={'size': 8},\n",
    "               loc='center left', \n",
    "               bbox_to_anchor=(1.0, 0.5))\n",
    "plt.setp(l.get_title(), multialignment='center')\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "plt.xlabel(\"Reservoir size\", fontsize=8)\n",
    "plt.ylabel(\"GPE in \\%\", fontsize=8)\n",
    "ax.set_ylim([27, None])\n",
    "plt.savefig('Basic_ESN_augmented_GPE.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(3.2, 1.3)\n",
    "sns.lineplot(data=df2, x=\"hidden\\_layer\\_size\", y=\"GPE\", hue=\"supervised\\_st\", marker=\"o\", ax=ax, legend=False)\n",
    "ax.set_xlim([0, 16200])\n",
    "ax.tick_params(labelsize=8)\n",
    "# l = plt.legend(title=r\"Pitch shift\\n up to [st]\", prop={'size': 8},loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "# plt.setp(l.get_title(), multialignment='center')\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "plt.xlabel(\"Reservoir size\", fontsize=8)\n",
    "plt.ylabel(\"GPE in \\%\", fontsize=8)\n",
    "ax.set_ylim([27, None])\n",
    "plt.savefig('KM_ESN_augmented_GPE.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
