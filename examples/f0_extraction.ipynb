{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f_{0}$ extraction using the Pitch Tracking Dataset from TU Graz (PTDBUG)\n",
    "\n",
    "At first, import packages to be used for the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "cwd = os.getcwd()\n",
    "module_path = os.path.dirname(cwd)  # target working directory\n",
    "\n",
    "sys.path = [item for item in sys.path if item != module_path]  # remove module_path from sys.path\n",
    "sys.path.append(module_path)  # add module_path to sys.path\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from scipy.stats import uniform\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import make_scorer\n",
    "from pyrcn.metrics import mean_squared_error\n",
    "from pyrcn.model_selection import SequentialSearchCV\n",
    "from pyrcn.util import FeatureExtractor\n",
    "from pyrcn.datasets import fetch_ptdb_tug_dataset\n",
    "from pyrcn.echo_state_network import SeqToSeqESNRegressor\n",
    "from pyrcn.base import InputToNode, PredefinedWeightsInputToNode, NodeToNode\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png', 'pdf')\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print number of files that are included in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_extraction_pipeline(sr=16000):\n",
    "    audio_loading = Pipeline([(\"load_audio\", FeatureExtractor(func=librosa.load, kw_args={\"sr\": sr, \"mono\": True})),\n",
    "                              (\"normalize\", FeatureExtractor(func=librosa.util.normalize, kw_args={\"norm\": np.inf}))])\n",
    "    \n",
    "    feature_extractor = Pipeline([(\"mel_spectrogram\", FeatureExtractor(func=librosa.feature.melspectrogram, \n",
    "                                                                       kw_args={\"sr\": sr, \"n_fft\": 1024, \"hop_length\": 160, \n",
    "                                                                                \"window\": 'hann', \"center\": False, \n",
    "                                                                                \"power\": 2.0, \"n_mels\": 80, \"fmin\": 40, \n",
    "                                                                                \"fmax\": 4000, \"htk\": True})),\n",
    "                                            (\"power_to_db\", FeatureExtractor(func=librosa.power_to_db, kw_args={\"ref\": 1}))])\n",
    "\n",
    "    feature_extraction_pipeline = Pipeline([(\"audio_loading\", audio_loading),\n",
    "                                            (\"feature_extractor\", feature_extractor)])\n",
    "    return feature_extraction_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the dataset\n",
    "\n",
    "This might require a large amount of time and memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "feature_extraction_pipeline = create_feature_extraction_pipeline()\n",
    "\n",
    "X_train, X_test, y_train, y_test = fetch_ptdb_tug_dataset(data_origin=\"Z:/Projekt-Pitch-Datenbank/SPEECH_DATA\", \n",
    "                                                          data_home=None, preprocessor=feature_extraction_pipeline, \n",
    "                                                          force_preprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature visualization. All features carry information, since the variance is always large\n",
    "\n",
    "We can fit a StandardScaler here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsplot(ax, data,**kw):\n",
    "    x = np.arange(data.shape[1])\n",
    "    est = np.mean(data, axis=0)\n",
    "    sd = np.std(data, axis=0)\n",
    "    cis = (est - sd, est + sd)\n",
    "    ax.fill_between(x,cis[0],cis[1],alpha=0.2, **kw)\n",
    "    ax.plot(x,est,**kw)\n",
    "    ax.margins(x=0)\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, sharex=True)\n",
    "fig.set_size_inches(4, 2.5)\n",
    "tsplot(axs[0], np.concatenate(np.hstack((X_train, X_test))))\n",
    "axs[0].set_xlabel('Feature Index')\n",
    "axs[0].set_ylabel('Magnitude')\n",
    "scaler = StandardScaler().fit(np.concatenate(X_train))\n",
    "for k, X in enumerate(X_train):\n",
    "    X_train[k] = scaler.transform(X=X)\n",
    "for k, X in enumerate(X_test):\n",
    "    X_test[k] = scaler.transform(X=X)\n",
    "tsplot(axs[1], np.concatenate(np.hstack((X_train, X_test))))\n",
    "axs[1].set_xlabel('Feature Index')\n",
    "axs[1].set_ylabel('Magnitude')\n",
    "plt.grid()\n",
    "# plt.savefig('features_statistics.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define several error functions for $f_{0}$ extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpe(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Gross pitch error:\n",
    "    \n",
    "    All frames that are considered voiced by both pitch tracker and ground truth, \n",
    "    for which the relative pitch error is higher than a certain threshold (\\SI{20}{\\percent}).\n",
    "    \n",
    "    \"\"\"\n",
    "    idx = np.nonzero(y_true*y_pred)[0]\n",
    "    return np.mean(np.abs(y_true[idx] - y_pred[idx]) > 0.2 * y_true[idx])\n",
    "\n",
    "\n",
    "def vde(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Voicing Decision Error:\n",
    "    \n",
    "    Proportion of frames for which an incorrect voiced/unvoiced decision is made.\n",
    "    \n",
    "    \"\"\"\n",
    "    return zero_one_loss(y_true, y_pred)\n",
    "\n",
    "\n",
    "def fpe(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Fine Pitch Error:\n",
    "    \n",
    "    Standard deviation of the distribution of relative error values (in cents) from the frames\n",
    "    that do not have gross pitch errors\n",
    "    \"\"\"\n",
    "    idx_voiced = np.nonzero(y_true * y_pred)[0]\n",
    "    idx_correct = np.argwhere(np.abs(y_true - y_pred) <= 0.2 * y_true).ravel()\n",
    "    idx = np.intersect1d(idx_voiced, idx_correct)\n",
    "    if idx.size == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 100 * np.std(np.log2(y_pred[idx] / y_true[idx]))\n",
    "\n",
    "\n",
    "def ffe(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    $f_{0}$ Frame Error:\n",
    "    \n",
    "    Proportion of frames for which an error (either according to the GPE or the VDE criterion) is made.\n",
    "    FFE can be seen as a single measure for assessing the overall performance of a pitch tracker.\n",
    "    \"\"\"\n",
    "    idx_correct = np.argwhere(np.abs(y_true - y_pred) <= 0.2 * y_true).ravel()\n",
    "    return 1 - len(idx_correct) / len(y_true)\n",
    "\n",
    "\n",
    "def custom_scorer(y_true, y_pred):\n",
    "    gross_pitch_error = [None] * len(y_true)\n",
    "    for k, (y_t, y_p) in enumerate(zip(y_true, y_pred)):\n",
    "        gross_pitch_error[k] = gpe(y_true=y_t[:, 0]*y_t[:, 1], y_pred=y_p[:, 0]*(y_p[:, 1] >= .5))\n",
    "    return np.mean(gross_pitch_error)\n",
    "\n",
    "gpe_scorer = make_scorer(custom_scorer, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a ESN\n",
    "\n",
    "To develop an ESN model for multipitch tracking, we need to tune several hyper-parameters, e.g., input_scaling, spectral_radius, bias_scaling and leaky integration.\n",
    "\n",
    "We follow the way proposed in the paper for multipitch tracking and for acoustic modeling of piano music to optimize hyper-parameters sequentially.\n",
    "\n",
    "We define the search spaces for each step together with the type of search (a grid search in this context).\n",
    "\n",
    "At last, we initialize a SeqToSeqESNRegressor with the desired output strategy and with the initially fixed parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initially_fixed_params = {'hidden_layer_size': 50,\n",
    "                          'k_in': 10,\n",
    "                          'input_scaling': 0.4,\n",
    "                          'input_activation': 'identity',\n",
    "                          'bias_scaling': 0.0,\n",
    "                          'spectral_radius': 0.0,\n",
    "                          'leakage':1.0,\n",
    "                          'k_rec': 10,\n",
    "                          'reservoir_activation': 'tanh',\n",
    "                          'bi_directional': False,\n",
    "                          'wash_out': 0,\n",
    "                          'continuation': False,\n",
    "                          'alpha': 1e-3,\n",
    "                          'random_state': 42}\n",
    "\n",
    "step1_esn_params = {'input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'spectral_radius': uniform(loc=0, scale=2)}\n",
    "\n",
    "step2_esn_params = {'leakage': loguniform(1e-5, 1e0)}\n",
    "step3_esn_params = {'bias_scaling': np.linspace(0.0, 1.0, 11)}\n",
    "step4_esn_params = {'alpha': loguniform(1e-5, 1e1)}\n",
    "\n",
    "kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': gpe_scorer}\n",
    "kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': gpe_scorer}\n",
    "kwargs_step3 = {'verbose': 1, 'n_jobs': -1, 'scoring': gpe_scorer}\n",
    "kwargs_step4 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': gpe_scorer}\n",
    "\n",
    "# The searches are defined similarly to the steps of a sklearn.pipeline.Pipeline:\n",
    "searches = [('step1', RandomizedSearchCV, step1_esn_params, kwargs_step1),\n",
    "            ('step2', RandomizedSearchCV, step2_esn_params, kwargs_step2),\n",
    "            ('step3', GridSearchCV, step3_esn_params, kwargs_step3),\n",
    "            ('step4', RandomizedSearchCV, step4_esn_params, kwargs_step4)]\n",
    "\n",
    "base_esn = SeqToSeqESNRegressor(**initially_fixed_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "We provide a SequentialSearchCV that basically iterates through the list of searches that we have defined before. It can be combined with any model selection tool from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    sequential_search = load(\"../sequential_search_f0_mel_50.joblib\")\n",
    "except FileNotFoundError:\n",
    "    print(FileNotFoundError)\n",
    "    sequential_search = SequentialSearchCV(base_esn, searches=searches).fit(X_train, y_train)\n",
    "    dump(sequential_search, \"../sequential_search_f0_mel_50.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_search.all_best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize hyper-parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step1\"])\n",
    "df[\"mean_test_score\"] = -df[\"mean_test_score\"]\n",
    "# df.mean_test_score = df.mean_test_score.clip(upper=1.5e-1)\n",
    "fig = plt.figure()\n",
    "ax = sns.scatterplot(x=\"param_spectral_radius\", y=\"param_input_scaling\", hue=\"mean_test_score\", palette='viridis', data=df)\n",
    "plt.xlabel(\"Spectral Radius\")\n",
    "plt.ylabel(\"Input Scaling\")\n",
    "\n",
    "norm = plt.Normalize(-0.8, -0.1)\n",
    "# sm = plt.cm.ScalarMappable(cmap=\"viridis\")\n",
    "sm = plt.cm.ScalarMappable(cmap=\"viridis\", norm=norm)\n",
    "sm.set_array([])\n",
    "plt.xlim((0, 2.01))\n",
    "plt.ylim((0, 1.03))\n",
    "\n",
    "# Remove the legend and add a colorbar\n",
    "ax.get_legend().remove()\n",
    "ax.figure.colorbar(sm, label='GPE')\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax.yaxis.set_major_locator(ticker.MaxNLocator(5))\n",
    "ax.xaxis.set_major_locator(ticker.MaxNLocator(5))\n",
    "plt.grid()\n",
    "plt.savefig('optimize_is_sr_50.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step2\"])\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax = sns.lineplot(data=df, x=\"param_leakage\", y=\"mean_test_score\")\n",
    "plt.xlabel(\"Leakage\")\n",
    "plt.ylabel(\"GPE\")\n",
    "# plt.xlim((0, 1))\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.4f'))\n",
    "plt.grid()\n",
    "plt.savefig('optimize_leakage_50.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step3\"])\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax = sns.lineplot(data=df, x=\"param_bias_scaling\", y=\"mean_test_score\")\n",
    "plt.xlabel(\"Bias Scaling\")\n",
    "plt.ylabel(\"GPE\")\n",
    "plt.xlim((0, 1))\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.5f'))\n",
    "plt.grid()\n",
    "plt.savefig('optimize_bias_scaling_50.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step4\"])\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax = sns.lineplot(data=df, x=\"param_alpha\", y=\"mean_test_score\")\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"GPE\")\n",
    "plt.xlim((0, 10))\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.5f'))\n",
    "plt.grid()\n",
    "plt.savefig('optimize_alpha_50.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$K$-Means initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fitting kmeans with features from the training set...\")\n",
    "t1 = time.time()\n",
    "kmeans = MiniBatchKMeans(n_clusters=50, n_init=200, reassignment_ratio=0, max_no_improvement=50, init='k-means++', verbose=2, random_state=0)\n",
    "kmeans.fit(X=np.concatenate(np.concatenate((X_train, X_test))))\n",
    "print(\"done in {0}!\".format(time.time() - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize an Echo State Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initially_fixed_params = {'hidden_layer_size': 50,\n",
    "                          'k_in': 10,\n",
    "                          'input_scaling': 0.4,\n",
    "                          'input_activation': 'identity',\n",
    "                          'bias_scaling': 0.0,\n",
    "                          'spectral_radius': 0.0,\n",
    "                          'leakage':1.0,\n",
    "                          'k_rec': 10,\n",
    "                          'reservoir_activation': 'tanh',\n",
    "                          'bi_directional': False,\n",
    "                          'wash_out': 0,\n",
    "                          'continuation': False,\n",
    "                          'alpha': 1e-3,\n",
    "                          'random_state': 42}\n",
    "\n",
    "if initially_fixed_params[\"hidden_layer_size\"] <=50:\n",
    "    w_in = np.divide(kmeans.cluster_centers_, np.linalg.norm(kmeans.cluster_centers_, axis=1)[:, None])\n",
    "else:\n",
    "    w_in = np.pad(np.divide(kmeans.cluster_centers_, np.linalg.norm(kmeans.cluster_centers_, axis=1)[:, None]), ((0, base_input_to_nodes.hidden_layer_size - 50), (0, 0)), mode='constant', constant_values=0)\n",
    "\n",
    "base_input_to_node = PredefinedWeightsInputToNode(predefined_input_weights=w_in.T, input_scaling=0.4)\n",
    "\n",
    "\n",
    "step1_esn_params = {'input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'spectral_radius': uniform(loc=0, scale=2)}\n",
    "\n",
    "step2_esn_params = {'leakage': loguniform(1e-5, 1e0)}\n",
    "step3_esn_params = {'bias_scaling': np.linspace(0.0, 1.0, 11)}\n",
    "step4_esn_params = {'alpha': loguniform(1e-5, 1e1)}\n",
    "\n",
    "kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': gpe_scorer}\n",
    "kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': gpe_scorer}\n",
    "kwargs_step3 = {'verbose': 1, 'n_jobs': -1, 'scoring': gpe_scorer}\n",
    "kwargs_step4 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': gpe_scorer}\n",
    "\n",
    "# The searches are defined similarly to the steps of a sklearn.pipeline.Pipeline:\n",
    "searches = [('step1', RandomizedSearchCV, step1_esn_params, kwargs_step1),\n",
    "            ('step2', RandomizedSearchCV, step2_esn_params, kwargs_step2),\n",
    "            ('step3', GridSearchCV, step3_esn_params, kwargs_step3),\n",
    "            ('step4', RandomizedSearchCV, step4_esn_params, kwargs_step4)]\n",
    "\n",
    "base_esn = SeqToSeqESNRegressor(input_to_node=base_input_to_node).set_params(**initially_fixed_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to load a pre-trained ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    sequential_search = load(\"../sequential_search_f0_mel_km_50.joblib\")\n",
    "except FileNotFoundError:\n",
    "    print(FileNotFoundError)\n",
    "    sequential_search = SequentialSearchCV(base_esn, searches=searches).fit(X_train, y_train)\n",
    "    dump(sequential_search, \"../sequential_search_f0_mel_km_50.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_search.all_best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize hyper-parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step1\"])\n",
    "df[\"mean_test_score\"] = -df[\"mean_test_score\"]\n",
    "# df.mean_test_score = df.mean_test_score.clip(upper=1.5e-1)\n",
    "fig = plt.figure()\n",
    "ax = sns.scatterplot(x=\"param_spectral_radius\", y=\"param_input_scaling\", hue=\"mean_test_score\", palette='viridis', data=df)\n",
    "plt.xlabel(\"Spectral Radius\")\n",
    "plt.ylabel(\"Input Scaling\")\n",
    "\n",
    "norm = plt.Normalize(-0.8, -0.1)\n",
    "# sm = plt.cm.ScalarMappable(cmap=\"viridis\")\n",
    "sm = plt.cm.ScalarMappable(cmap=\"viridis\", norm=norm)\n",
    "sm.set_array([])\n",
    "plt.xlim((0, 2.01))\n",
    "plt.ylim((0, 1.03))\n",
    "\n",
    "# Remove the legend and add a colorbar\n",
    "ax.get_legend().remove()\n",
    "ax.figure.colorbar(sm, label='GPE')\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax.yaxis.set_major_locator(ticker.MaxNLocator(5))\n",
    "ax.xaxis.set_major_locator(ticker.MaxNLocator(5))\n",
    "plt.grid()\n",
    "plt.savefig('optimize_is_sr_km_50.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step2\"])\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax = sns.lineplot(data=df, x=\"param_leakage\", y=\"mean_test_score\")\n",
    "plt.xlabel(\"Leakage\")\n",
    "plt.ylabel(\"GPE\")\n",
    "# plt.xlim((0, 1))\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.4f'))\n",
    "plt.grid()\n",
    "plt.savefig('optimize_leakage_km_50.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step3\"])\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax = sns.lineplot(data=df, x=\"param_bias_scaling\", y=\"mean_test_score\")\n",
    "plt.xlabel(\"Bias Scaling\")\n",
    "plt.ylabel(\"GPE\")\n",
    "plt.xlim((0, 1))\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.5f'))\n",
    "plt.grid()\n",
    "plt.savefig('optimize_bias_scaling_km_50.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sequential_search.all_cv_results_[\"step4\"])\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(2, 1.25)\n",
    "ax = sns.lineplot(data=df, x=\"param_alpha\", y=\"mean_test_score\")\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"GPE\")\n",
    "plt.xlim((0, 10))\n",
    "tick_locator = ticker.MaxNLocator(5)\n",
    "ax.xaxis.set_major_locator(tick_locator)\n",
    "ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.5f'))\n",
    "plt.grid()\n",
    "plt.savefig('optimize_alpha_km_50.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_y = np.vstack((np.concatenate(y_train), np.concatenate(y_test)))[:, 0]\n",
    "np.min(all_y[all_y!=0]), np.max(all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.histplot(x=all_y[all_y != 0], log_scale=True, stat=\"density\")\n",
    "ax.set(xlabel=r'$f_{0}$ in Hertz', ylabel='Density')\n",
    "ax.xaxis.set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "ax.set_xticks([50, 100, 200, 400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
